\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{tikz}
% Corporate Design of the University of Tübingen
% Primary Colors
\definecolor{TUred}{RGB}{165,30,55}
\definecolor{TUgold}{RGB}{180,160,105}
\definecolor{TUdark}{RGB}{50,65,75}
\definecolor{TUgray}{RGB}{175,179,183}

% Secondary Colors
\definecolor{TUdarkblue}{RGB}{65,90,140}
\definecolor{TUblue}{RGB}{0,105,170}
\definecolor{TUlightblue}{RGB}{80,170,200}
\definecolor{TUlightgreen}{RGB}{130,185,160}
\definecolor{TUgreen}{RGB}{125,165,75}
\definecolor{TUdarkgreen}{RGB}{50,110,30}
\definecolor{TUocre}{RGB}{200,80,60}
\definecolor{TUviolet}{RGB}{175,110,150}
\definecolor{TUmauve}{RGB}{180,160,150}
\definecolor{TUbeige}{RGB}{215,180,105}
\definecolor{TUorange}{RGB}{210,150,0}
\definecolor{TUbrown}{RGB}{145,105,70}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{float}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Comparing different active learning query methods}

\begin{document}

\twocolumn[
\icmltitle{TODO: Funky active learning pun}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
	\icmlauthor{Lukas Weber}{first}
\end{icmlauthorlist}

\icmlaffiliation{first}{MSc Computer Science, Tübingen, GER}
\icmlcorrespondingauthor{Lukas Weber}{lukas2.weber@student.uni-tuebingen.de}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, Active Learning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
	A lot of unlabelled data generally available but annotation is costly. A goal in fields where labelled data is not available  is to maximize performance with minimum amount of labeled data instances. \\
	Active learning selects instances for labelling based on a selection strategy. 
	\begin{itemize}
		\item Objective: Compare active learning query methods using modAL.
		\item Methods: Experiments with Logistic Regression (MNIST) and a CNN (CIFAR).
		\item Experiments: Vary initial dataset sizes (small, moderate, large).
		\item Key findings: Highlight main conclusions about performance in different scenarios.
	\end{itemize}
\end{abstract}


\section{Introduction and Related Work}\label{sec:intro}
Supervised learning often requires large, annotated datasets, but labeling data can be costly and time-intensive, especially when expert knowledge is needed. 
Active learning addresses this challenge of reducing the overall labeling costs through the selection of the most informative samples for annotation based on a query strategy. This approach enables machine learning models to perform well in tasks with limited data and high labeling costs. \\
The effectiveness of active learning depends heavily on the chosen query strategy, which determines which samples are selected for annotation. Two main frameworks are commonly used:
\begin{enumerate}
	\item Pool-based active learning: A model is trained on a small labeled dataset and used to evaluate a large pool of unlabeled data. The most informative samples are selected based on a query strategy, labeled by an oracle, and added to the training set. The model is retrained with the updated training set and the process is repeated until a stopping criterion is met.
	\item Stream-based active learning: Here, unlabeled data arrives sequentially and the model decides whether to query the label of an incoming instance using an evaluation metric like the uncertainty of its prediction and checking whether a certain threshold is reached. If not, the instance is discarded.
\end{enumerate}
This report focuses on comparing several query strategies for both pool-based and stream-based active learning. Experiments in this paper leverage the modular Active Learning framework (modAL)~\cite{danka_modalmodularactivelearning}, which simplified the implementation of active learning pipelines, includes multiple pre-implemented query strategies and is compatible with models from the Scikit-learn library~\cite{scikit-learn}. \textcolor{red}{Convolutional Neural Netowrk (CNN) implemented with PyTorch(ADD CITATION) and transformed with skorch~\cite{skorch} to make it compatible with modAL as it is only compatible with sklearn models.} \\
Several previous works have explored and compared different query methods across various domains. \\
\cite{schröder_revisitinguncertaintybasedquerystrategies} compare various uncertainty-based query strategies in the context of fine-tuning transformer models in text classification tasks. \cite{zhan_comparativesurveydeepactive} on the other hand compares several query strategies on the image datasets MNIST and CIFAR. \\
According to \cite{ueno_benchmarkingofquerystrategies} most papers concentrate on two image-based datasets: MNIST~\cite{mnist} and CIFAR~\cite{cifar}, while their paper evaluates strategies on six different datasets, including medical and visual inspection images. They also examine strategies independent of training processes to minimize biases from early-stage randomness or underfitting. This is shown in an experiment where fully trained models are used to select the most informative samples based on several different query strategies to construct a labeled dataset. \\ 
A recurring issue in active learning research is the lack of standardization in experimental setups, as noticed additionally by~\cite{werner_comparableactivelearning}. This inconsistency complicates the comparability of existing and novel query strategies. To address this, they propose a benchmark framework for evaluatting active learning strategies across tabular, image and text datasets. This framework uses robust evaluation protocols to reduce variance and ensure comparability. \\
In contrast, \cite{ueno_benchmarkingofquerystrategies} focus specifically on image-based datasets from diverse domains such as medical or visual inspection images. While~\cite{werner_comparableactivelearning} focuses on a wide range of data-domains including images, text and vector-based datasets, this report focuses on comparing strategies in the image domain. 
It compares query strategies for both models such as simple logistic regression on MNIST and more complex architectures, like a CNN on CIFAR-10 to study the effect of model complexity on the optimal query strategy. \\
It is uncertain whether well working query strategies generalize well from simple to more complex models like Neural Networks~\cite{schröder_surveyactivelearningtext}. Understanding this generalizability is crucial for real-world applications where model complexity varies across tasks and datasets. \\
Furthermore, this report investigates the impact of the initial labeled dataset size, as its impact in combination with model complexity is rarely explored. 
By examining scenarios with varying amounts of initial labeled data, this work simulates various real-world applications: (1) starting with no labeled data, (2) having a small pool of labeled data, and (3) working with a well-trained model to identify data points for fine-tuning.
\\
\\
The contributions of this paper are as follows:
\begin{itemize}
	\item \textbf{Systematic evaluation of active learning query strategies} across different datasets and model complexities, using MNIST with logistic regression and CIFAR-10 with a CNN.
	\item \textbf{Investigation of the incluence of model complexity} on the effectiveness of query strategies to provide insights into how shallow and deep models respond differently to active learning methods.
	\item \textbf{Analysis of impact of initial labeled dataset size} on the performance of query strategies, simulating possible real-world scenarios such as starting from scratch, having a small labeled dataset or fine-tuning a pre-trained model.
\end{itemize}
Section~\ref{sec:methods} outlines the used datasets, processing steps and models. Section~\ref{sec:experiments} describes the experimental design, followed by the results and discussion in sections~\ref{sec:results} and~\ref{sec:discussion} respectively. The report concludes with key findings in section~\ref{sec:conclusion}

\section{Methods}\label{sec:methods}
Several different datasets are used in the following experiments.
\subsection{Datasets}
MNIST is a dataset consisting of handwritten digits with 60.000 train samples and 10.000 test samples. Digits are size-normalized and centered in a $28 \times 28$ image~\cite{mnist}. Each digits belongs to one of 10 classes, corresponding from 0 to 9. \\
Upon loading the dataset, each image is vectorized, i.e. transformed from a $28 \times 28$ sized image-matrix to a vector of length $784$. Furthermore, each pixel contains a grayscale-value from 0 to 255. Pixel values are normalized to a range between 0 and 1 to provide a value range that works well with logistic regression.
\\
\\
CIFAR-10 is a dataset which consists of 50.000 train- and 10.000 test-images. Each natural image belongs to one of 10 classes. Each class has exactly 5.000 train and 1.000 test images. The image-size is $32 \times 32$ pixels. 
\textcolor{red}{CIFAR preprocessing split}. CIFAR-10 includes real-world images and is thus expected to be more complicated to learn.

Both used datasets are balanced, i.e. each class has roughly the same number of samples as any other class. Furthermore, 20\% of both train sets are used for 5-fold-cross-validation, resulting in a test set size of $48.000$ train images for MNIST and $40.000$ train images for CIFAR.

\subsection{Multinomial Logistic Regression Model}
Sklearn's LogisticRegression module is used to predict digits on MNIST. Multinomial logistic regression predicts a probability for each class instead of a single probability as in Binomial logistic regression.
\\
The model estimates the probability $P(y=i \mid X)$ of a sample belonging to class $i$ using the SoftMax function:
\begin{align}
	P(y=i \mid X)=\frac{\exp \left(X \beta_i\right)}{\sum_{j=1}^k \exp \left(X \beta_j\right)}
\end{align}
where $\beta_i$ are the coefficients for class $i$. The SoftMax function ensures that probabilities across classes sum to 1.

The model maximizes the likelihood of the observed data:
\begin{align}
	\mathcal{L}(\beta)=\prod_{n=1}^N \prod_{i=1}^k\left(P\left(y_n=i \mid X_n\right)\right)^{y_{n i}}
\end{align}
where $N$ is the number of samples, and $y_{n i}$ indicates if sample $n$ belongs to class $i$. \textcolor{red}{Should I also explain Cross-Entropy loss? $\begin{aligned}
		&\text { For a multinomial logistic regression model with } k \text { classes, the cross-entropy loss } \mathcal{L} \text { is defined as: }\\
		&\mathcal{L}=-\frac{1}{N} \sum_{n=1}^N \sum_{j=1}^k y_{n j} \log \left(P\left(y=j \mid X_n\right)\right)
\end{aligned}$}\\
Regularization using the $\ell_2$-norm prevents overfitting with a strength $C=0.01$. The lbfgs-solver, with a maximum iteration of 500, is used for efficient gradient-based optimization. Early-stopping with a tolerance of 0.001 is used. \\

\subsection{Convolutional Neural Network}
The model which is used to learn the classes in CIFAR-10 is a CNN. It comprises of two convolutional layers followed by three fully connected layers. \\
The first convolutional layer processes the input image with 3 input and $l1\_channels$ output channels, followed by a ReLU activation. The second convolutional layer further processes the output from the first layer with $l1\_channels$ input and $l2\_channels$ output channels, applying ReLU activation, max pooling, and dropout for regularization.
\\
The output from the convolutional layers is flattened and passed through three fully connected layers. The first two fully connected layers are followed by ReLU activations and dropout for regularization. The first fully connected layer has an output size of $l4_input$ and the second layer has an output size of $l5_input$ neurons. The final output layer has 10 neurons, corresponding to the number of classes in the CIFAR dataset.
\\
The model is implemented using PyTorch\textcolor{red}{cite} and skorch~\cite{skorch} is used to make it possible to use it with Sklearn, allowing integration with the modAL active learning framework.
\\
\textcolor{red}{Early stopping?}
\\
During training, PyTorch's CrossEntropyLoss is used. 

\subsection{Active Learning}
\begin{itemize}
	\item During active learning with pool-based query strategies in the case of logistic regression, 5 instances are queried at a time for 250 iterations resulting in a labeled pool which is increased by $1.250$ samples. \\
	When training the CNN, 250 instances are chosen for 50 iterations. This enlarges the labeled pool by $12.500$ samples. \\
	In stream-based methods, the number of queries is adapted to match the given numbers and guarantee that the labeled dataset size is the same as in the pool based methods. 
	\item Used active learning query strategies include pool- and stream-based methods. The pool based methods include uncertainty sampling, uncertainty margin sampling and uncertainty entropy sampling. Stream-based active learning is done with uncertainty as the evaluation metric deciding whether to use samples or not. Additionally, random sampling was used to serve as a reference point.  
	\textcolor{red}{maybe explain the query strategies more in detail}
	\item After querying instances and retraining, models are not re-initialized but rather fine-tuned. 
\end{itemize}

\section{Experiments}\label{sec:experiments}
Several experiments are conducted with the two model-dataset combinations to research different scenarios for which active learning can be used.
\begin{itemize}
	\item Experiment 1: Starts with a minimal amount of data. In both cases these are 10 samples.
	\item Experiment 2: Starts with $15\%$ of the train dataset. 
	\item Experiment 3: Starts with $80\%$ of the train dataset.
\end{itemize}
In experiment 1, a random sample is chosen for each class in the datasets. \\
In each experiment, the random samples 

For reproducibility purposes, the same random seed $42$ was used to initialize all random number generators and model weights to ensure that same configurations lead to the same outcomes, even when training models in succession. 

For each query method one model is trained. (I did not perform multiple runs)
Before starting the active learning process, each model is pre-trained using the samples in the initial labeled dataset for \textcolor{red}{X epochs}. In the case of pool-based query methods, 5 samples are queried for 250 iterations when training with samples from MNIST and 250 samples are queried for 50 iterations when training with instances from CIFAR. 
\\
\begin{itemize}
	\item Batch size for CNN is 128.
	\item Evaluation metrics. Metrics for evaluation (Accuracy, train error, confusion matrix, ...)
	\item How are the splits (initial pool, initial training data) generated
	\item initial labeled pool sizes
\end{itemize}
	
	

Scenarios
\begin{itemize}
	\item describe experiments and their settings and purposes
\end{itemize}

\section{Results}\label{sec:results}
Quantitative results: Tables or plots comparing the query strategies across scenarios. Also maybe key perforamnce differences (accuracy vs number of labeled examples)

\section{Discussion}\label{sec:discussion}
Qualitative analysis: Why certain strategies perform better or worse in specific settings. Trends between simple models and complex models
\\
\\
Impact of initial dataset size
\\
\\
Most methods require knowledge about the balance of the dataset and might be disadvantageous to use in case of a disbalanced dataset. In real world scenarios, this is not known. Both used datasets are balanced so the impact of query strategies on imbalanced datasets was not tested. \\
Only image-based datasets were tested. Not vector- or text-based. 

Only one network is trained for each query method. This could be improved upon when training multiple models.

\section{Conclusion}\label{sec:conclusion}
Summary of findings and insights (maybe that a strategy works best with small datasets while another one is robust on bigger ones)
\\
Implications: practical recommendations?
\\
Future work? (More diverse datasets, exploring additional strategies, applying in real-world tasks)

\newpage

\section{Cool ideas}
\begin{itemize}
	\item Show the typical framework of active learning in a figure in the introduction
	\item Accuracy vs. fraction of data size in relation to the datset (percentage of used data points)
\end{itemize}

\section{Good formulations}
From the active learning survey: Entropy is an information-theoretic
measure that represents the amount of information needed to “encode” a distri-
bution. As such, it is often thought of as a measure of uncertainty or impurity in machine learning.

\section{Comparison to the other papers}
% FROM HERE THIS IS NOT GOING TO BE PART OF THE FINAL PART
Previous papers do not: Is everything here now mentioned previously? if yes, then it can be deleted.
\begin{itemize}
	\item \cite{schröder_revisitinguncertaintybasedquerystrategies}: Does not compare across different sizes of the pool, and for simple models.
	\item \cite{comparativesurveydeepactive}: Does not compare impact of different initial train set size and strategies on shallow classifiers.
	
	\item According to \cite{ueno_benchmarkingofquerystrategies}: \textcolor{red}{check if true:} Does not compare different sizes of the initial starting sets as well as how simple and more complex perform with the query methods
	
	\item \cite{werner_comparableactivelearning}: They focus on a wide range of domains, this paper only compares methods in the image domain. I compare strategies for a simple logistic regression model and a complex CNN across different starting sizes of the annotated dataset. \\
\end{itemize}
% TO HERE

\bibliography{bibliography}
\bibliographystyle{icml2023}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
