\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{tikz}
% Corporate Design of the University of Tübingen
% Primary Colors
\definecolor{TUred}{RGB}{165,30,55}
\definecolor{TUgold}{RGB}{180,160,105}
\definecolor{TUdark}{RGB}{50,65,75}
\definecolor{TUgray}{RGB}{175,179,183}

% Secondary Colors
\definecolor{TUdarkblue}{RGB}{65,90,140}
\definecolor{TUblue}{RGB}{0,105,170}
\definecolor{TUlightblue}{RGB}{80,170,200}
\definecolor{TUlightgreen}{RGB}{130,185,160}
\definecolor{TUgreen}{RGB}{125,165,75}
\definecolor{TUdarkgreen}{RGB}{50,110,30}
\definecolor{TUocre}{RGB}{200,80,60}
\definecolor{TUviolet}{RGB}{175,110,150}
\definecolor{TUmauve}{RGB}{180,160,150}
\definecolor{TUbeige}{RGB}{215,180,105}
\definecolor{TUorange}{RGB}{210,150,0}
\definecolor{TUbrown}{RGB}{145,105,70}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{float}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Comparing different active learning query methods}

\begin{document}

\twocolumn[
\icmltitle{TODO: Funky active learning pun}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
	\icmlauthor{Lukas Weber}{first}
\end{icmlauthorlist}

\icmlaffiliation{first}{MSc Computer Science, Tübingen, GER}
\icmlcorrespondingauthor{Lukas Weber}{lukas2.weber@student.uni-tuebingen.de}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, Active Learning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
	A lot of unlabelled data generally available but annotation is costly. A goal in fields where labelled data is not available  is to maximize performance with minimum amount of labeled data instances. \\
	Active learning selects instances for labelling based on a selection strategy. 
	\begin{itemize}
		\item Objective: Compare active learning query methods using modAL.
		\item Methods: Experiments with Logistic Regression (MNIST) and a CNN (CIFAR).
		\item Experiments: Vary initial dataset sizes (small, moderate, large).
		\item Key findings: Highlight main conclusions about performance in different scenarios.
	\end{itemize}
\end{abstract}


\section{Introduction and Related Work}\label{sec:intro}
Supervised learning often requires large, annotated datasets, but labeling data can be costly and time-intensive, especially when expert knowledge is needed. 
Active learning addresses this challenge of reducing the overall labeling costs through the selection of the most informative samples for annotation based on a query strategy. This approach enables machine learning models to perform well in tasks with limited data and high labeling costs. \\
The effectiveness of active learning depends heavily on the chosen query strategy, which determines which samples are selected for annotation. Two main frameworks are commonly used:
\begin{enumerate}
	\item Pool-based active learning: A model is trained on a small labeled dataset and used to evaluate a large pool of unlabeled data. The most informative samples are selected based on a query strategy, labeled by an oracle, and added to the training set. The model is retrained with the updated training set and the process is repeated until a stopping criterion is met.
	\item Stream-based active learning: Here, unlabeled data arrives sequentially and the model decides whether to query the label of an incoming instance using an evaluation metric like the uncertainty of its prediction and checking whether a certain threshold is reached. If not, the instance is discarded.
\end{enumerate}
This report focuses on comparing several query strategies for both pool-based and stream-based active learning. Experiments in this paper leverage the modular Active Learning framework (modAL)~\cite{danka_modalmodularactivelearning}, which simplified the implementation of active learning pipelines, includes multiple pre-implemented query strategies and is compatible with models from the Scikit-learn library~\cite{scikit-learn}. \\
Several previous works have explored and compared different query methods across various domains. \\
\cite{schröder_revisitinguncertaintybasedquerystrategies} compare various uncertainty-based query strategies in the context of fine-tuning transformer models in text classification tasks. \cite{zhan_comparativesurveydeepactive} on the other hand compares several query strategies on the image datasets MNIST and CIFAR. \\
According to \cite{ueno_benchmarkingofquerystrategies} most papers concentrate on two image-based datasets: MNIST~\cite{mnist} and CIFAR~\cite{cifar}, while their paper evaluates strategies on six different datasets, including medical and visual inspection images. They also examine strategies independent of training processes to minimize biases from early-stage randomness or underfitting. This is shown in an experiment where fully trained models are used to select the most informative samples based on several different query strategies to construct a labeled dataset. \\ 
A recurring issue in active learning research is the lack of standardization in experimental setups, as noticed additionally by~\cite{werner_comparableactivelearning}. This inconsistency complicates the comparability of existing and novel query strategies. To address this, they propose a benchmark framework for evaluatting active learning strategies across tabular, image and text datasets. This framework uses robust evaluation protocols to reduce variance and ensure comparability. \\
In contrast, \cite{ueno_benchmarkingofquerystrategies} focus specifically on image-based datasets from diverse domains such as medical or visual inspection images. While~\cite{werner_comparableactivelearning} focuses on a wide range of data-domains including images, text and vector-based datasets, this report focuses on comparing strategies in the image domain. 
It compares query strategies for both models such as simple logistic regression on MNIST and more complex architectures, like a CNN on CIFAR-10 to study the effect of model complexity on the optimal query strategy. \\
It is uncertain whether well working query strategies generalize well from simple to more complex models like Neural Networks~\cite{schröder_surveyactivelearningtext}. Understanding this generalizability is crucial for real-world applications where model complexity varies across tasks and datasets. \\
Furthermore, this report investigates the impact of the initial labeled dataset size, as its impact in combination with model complexity is rarely explored. 
By examining scenarios with varying amounts of initial labeled data, this work simulates various real-world applications: (1) starting with no labeled data, (2) having a small pool of labeled data, and (3) working with a well-trained model to identify data points for fine-tuning.
\\
The contributions of this paper are as follows:
\begin{itemize}
	\item \textbf{Systematic evaluation of active learning query strategies} across different datasets and model complexities, using MNIST with logistic regression and CIFAR-10 with a CNN.
	\item \textbf{Investigation of the incluence of model complexity} on the effectiveness of query strategies to provide insights into how shallow and deep models respond differently to active learning methods.
	\item \textbf{Analysis of impact of initial labeled dataset size} on the performance of query strategies, simulating possible real-world scenarios such as starting from scratch, having a small labeled dataset or fine-tuning a pre-trained model.
\end{itemize}
Section~\ref{sec:methods} outlines the used datasets, processing steps and models. Section~\ref{sec:experiments} describes the experimental design, followed by the results and discussion in sections~\ref{sec:results} and~\ref{sec:discussion} respectively. The report concludes with key findings in section~\ref{sec:conclusion}

\section{Methods}\label{sec:methods}
\label{data_and_methods}
\subsection{Datasets}
\begin{itemize}
	\item MNIST and CIFAR describe
	\item processing steps \\
	MNIST: Vectorized, flattened (describe input size), normalized
\end{itemize}

\subsection{Models}
\begin{itemize}
	\item Logistic Regression: Details on implementation and hyperparameters, how were they chosen?
	\item same for CNN
\end{itemize}

\subsection{Active Learning}
\begin{itemize}
	\item How are the splits (initial pool, initial training data) generated
	\item numbmer of iterations for active learning
	\item Used active learning query strategies
\end{itemize}

Metrics for evaluation (Accuracy, train error, confusion matrix, ...)

\section{Experiments}\label{sec:experiments}
Setup: details for reproducibility (random number generators)?
Random seed = 42
\cite{comparableactivelearning} mention reproducibility problems in models even when setting random number generators before each run. By training models multiple times in succession and checking whether the same configurations lead to different outcomes, it is shown that in this scenario, reproducibility is guaranteed. \textcolor{red}{TODO: Schnell ein skript aufsetzen und dann finale accuracy und so weiter vergleichen.}
\\
Did I perform multiple runs?
\\

Scenarios
\begin{itemize}
	\item describe experiments and their settings and purposes
\end{itemize}

\section{Results}\label{sec:results}
Quantitative results: Tables or plots comparing the query strategies across scenarios. Also maybe key perforamnce differences (accuracy vs number of labeled examples)

\section{Discussion}\label{sec:discussion}
Qualitative analysis: Why certain strategies perform better or worse in specific settings. Trends between simple models and complex models
\\
\\
Impact of initial dataset size
\\
\\
Most methods require knowledge about the balance of the dataset and might be disadvantageous to use in case of a disbalanced dataset. In real world scenarios, this is not known. \\
Only image-based datasets were tested. Not vector- or text-based. 

\section{Conclusion}\label{sec:conclusion}
Summary of findings and insights (maybe that a strategy works best with small datasets while another one is robust on bigger ones)
\\
Implications: practical recommendations?
\\
Future work? (More diverse datasets, exploring additional strategies, applying in real-world tasks)

\newpage

\section{Cool ideas}
\begin{itemize}
	\item Show the typical framework of active learning in a figure in the introduction
	\item Accuracy vs. fraction of data size in relation to the datset (percentage of used data points)
\end{itemize}

\section{Good formulations}
From the active learning survey: Entropy is an information-theoretic
measure that represents the amount of information needed to “encode” a distri-
bution. As such, it is often thought of as a measure of uncertainty or impurity in machine learning.

\section{Comparison to the other papers}
% FROM HERE THIS IS NOT GOING TO BE PART OF THE FINAL PART
Previous papers do not: Is everything here now mentioned previously? if yes, then it can be deleted.
\begin{itemize}
	\item \cite{schröder_revisitinguncertaintybasedquerystrategies}: Does not compare across different sizes of the pool, and for simple models.
	\item \cite{comparativesurveydeepactive}: Does not compare impact of different initial train set size and strategies on shallow classifiers.
	
	\item According to \cite{ueno_benchmarkingofquerystrategies}: \textcolor{red}{check if true:} Does not compare different sizes of the initial starting sets as well as how simple and more complex perform with the query methods
	
	\item \cite{werner_comparableactivelearning}: They focus on a wide range of domains, this paper only compares methods in the image domain. I compare strategies for a simple logistic regression model and a complex CNN across different starting sizes of the annotated dataset. \\
\end{itemize}
% TO HERE

\bibliography{bibliography}
\bibliographystyle{icml2023}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
