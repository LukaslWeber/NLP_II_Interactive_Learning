\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{tikz}
% Corporate Design of the University of Tübingen
% Primary Colors
\definecolor{TUred}{RGB}{165,30,55}
\definecolor{TUgold}{RGB}{180,160,105}
\definecolor{TUdark}{RGB}{50,65,75}
\definecolor{TUgray}{RGB}{175,179,183}

% Secondary Colors
\definecolor{TUdarkblue}{RGB}{65,90,140}
\definecolor{TUblue}{RGB}{0,105,170}
\definecolor{TUlightblue}{RGB}{80,170,200}
\definecolor{TUlightgreen}{RGB}{130,185,160}
\definecolor{TUgreen}{RGB}{125,165,75}
\definecolor{TUdarkgreen}{RGB}{50,110,30}
\definecolor{TUocre}{RGB}{200,80,60}
\definecolor{TUviolet}{RGB}{175,110,150}
\definecolor{TUmauve}{RGB}{180,160,150}
\definecolor{TUbeige}{RGB}{215,180,105}
\definecolor{TUorange}{RGB}{210,150,0}
\definecolor{TUbrown}{RGB}{145,105,70}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{float}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Comparing different active learning query methods}

\begin{document}

\twocolumn[
\icmltitle{TODO: Funky active learning pun}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
	\icmlauthor{Lukas Weber}{first}
\end{icmlauthorlist}

\icmlaffiliation{first}{MSc Computer Science, Tübingen, GER}
\icmlcorrespondingauthor{Lukas Weber}{lukas2.weber@student.uni-tuebingen.de}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, Active Learning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
	A lot of unlabelled data generally available but annotation is costly. A goal in fields where labelled data is not available  is to maximize performance with minimum amount of labeled data instances. \\
	Active learning selects instances for labelling based on a selection strategy. 
	\begin{itemize}
		\item Objective: Compare active learning query methods using modAL.
		\item Methods: Experiments with Logistic Regression (MNIST) and a CNN (CIFAR).
		\item Experiments: Vary initial dataset sizes (small, moderate, large).
		\item Key findings: Highlight main conclusions about performance in different scenarios.
	\end{itemize}
\end{abstract}


\section{Introduction}\label{sec:intro}
Complex tasks in supervised learning usually require large annotated datasets. However, the cost and time needed to label samples can be high, especially when expert knowledge in the according field is required.  \\
Active learning aims to reduce the overall labeling cost by selecting the most informative samples for annotation by an oracle and thus enable using machine learning for tasks where annotated data is sparse. \\
The effectiveness of active learning relies heavily on the choice of query strategy, as it determines which samples are selected for annotation and thus directly influences model performance. In pool-based active learning, a model is trained on a small labeled dataset and used to evaluate a large pool of unlabeled data. Informative samples are selected based on a query strategy, labeled by an oracle, and added to the training set. The model is retrained, and the process is repeated until a stopping criterion is met. \\ 
In stream-based active learning, unlabeled data arrives sequentially. Thus, the model decides whether to query the label of an incoming instance using an evaluation metric like the uncertainty of the prediction and checking whether a threshold is reached. If not, the instance is discarded. \\
This report focuses on comparing several pool-based as well as stream-based query strategies. \\
\textbf{Related Work:} Several previous works explore different query methods for several tasks. \cite{revisitinguncertaintybasedquerystrategies} compares various uncertainty based query strategies in the context of fine-tuning transformer models for text classification. 
\begin{itemize}
	\item Previous works have explored strategies such as ..., .., .... While they have been widely tested, most evaluations focus on  single dataset scenarios or specific models
	\item Then the gaps: However, few studies analyze the impact of initial dataset size on query strategy performance, and even fewer investigate their behavior on simple versus complex models.
\end{itemize}
Previous papers:
\begin{itemize}
	\item \cite{revisitinguncertaintybasedquerystrategies}: Does not compare across different sizes of the pool, and for simple models.
	\item \cite{benchmarkingofquerystrategies} Most papers concentrate on cifar or mnist, this paper evaluates six different datasets, including medical and visual inspection images. Also tackles the problem that experimental settings are not standardized, making evaluation of existing methods difficult. Also did a verification experiment where fully trained models are used to select most informative samples according to various query strategies and construct a labeled dataset. This is done to isolate and evaluate the effectiveness of query strategies indepently of training process/ underfitting or randomness due to early-stage training. 

	\item \cite{comparativesurveydeepactive}: compares different query methods on mnist and cifar with a cnn (resnet18). Does not compare impact of different initial train set size and strategies on shallow classifiers.


	\item \cite{comparableactivelearning} highlight critical challenges in active learning research, including the lack of reproducible experiments and fair comparisons across domains. They propose a benchmark framework evaluating active learning strategies on tabular, image, and text datasets using robust evaluation protocols to mitigate variance and ensure comparability. While their work focuses on a wide range of domais, this paper only compares methods in the image domain. Furthermore, this paper focuses on the image domain. Specifically, it compares strategies for a simple logistic regression model and a CNN across different starting sizes of the annotated dataset. \\
	By examining different starting sizes, this work simulates various real-world scenarios: (1) starting with no labeled data, (2) having a small pool of labeled data, and (3) working with a well-trained model to identify data points for fine-tuning.
\end{itemize}

\textbf{Modal}: description and relevance for research maybe? The modAL framework(ref) has simplified the implementation of active learning pipelines, yet its comparative use across different strategies remains underexplored.

Prior studies
\begin{itemize}
	\item existing studies that compare active learning methods
	\item do they have gaps in the literature? (maybe: comparison under different dataset sizes, use with cnns, comparison of simple or complicated models)
\end{itemize}

\textbf{Contributions}
Comparative study of popular active learning query strategies using modAL framework. Contributions are as follows:
\begin{itemize}
	\item systemativ evaluation on two datasets: MNIST (Logistic Regression) and CIFAR (CNN)
	\item analysis across different scenarios: small, moderate and large initial datasets
	\item insight into performance impact of model complexity and initial dataset size on active learning performance
\end{itemize}

\textbf{Structure of the paper}
\begin{itemize}
	\item outline paper
\end{itemize}

\section{Methods}\label{sec:methods}
\label{data_and_methods}
\subsection{Datasets}
\begin{itemize}
	\item MNIST and CIFAR describe
	\item processing steps \\
	MNIST: Vectorized, flattened (describe input size), normalized
\end{itemize}

\subsection{Models}
\begin{itemize}
	\item Logistic Regression: Details on implementation and hyperparameters, how were they chosen?
	\item same for CNN
\end{itemize}

\subsection{Active Learning}
\begin{itemize}
	\item How are the splits (initial pool, initial training data) generated
	\item numbmer of iterations for active learning
	\item Used active learning query strategies
\end{itemize}

Metrics for evaluation (Accuracy, train error, confusion matrix, ...)

Tool: modAL, scikit

\section{Experiments}\label{sec:experiments}
Setup: details for reproducibility (random number generators)?
Random seed = 42
\cite{comparableactivelearning} mention reproducibility problems in models even when setting random number generators before each run. By training models multiple times in succession and checking whether the same configurations lead to different outcomes, it is shown that in this scenario, reproducibility is guaranteed. \textcolor{red}{TODO: Schnell ein skript aufsetzen und dann finale accuracy und so weiter vergleichen.}
\\
Did I perform multiple runs?
\\

Scenarios
\begin{itemize}
	\item describe experiments and their settings and purposes
\end{itemize}

\section{Results}\label{sec:results}
Quantitative results: Tables or plots comparing the query strategies across scenarios. Also maybe key perforamnce differences (accuracy vs number of labeled examples)

\section{Discussion}\label{sec:discussion}
Qualitative analysis: Why certain strategies perform better or worse in specific settings. Trends between simple models and complex models
\\
\\
Impact of initial dataset size
\\
\\
Most methods require knowledge about the balance of the dataset and might be disadvantageous to use in case of a disbalanced dataset. In real world scenarios, this is not known. \\
Only image-based datasets were tested. Not vector- or text-based. 

\section{Conclusion}\label{sec:conclusion}
Summary of findings and insights (maybe that a strategy works best with small datasets while another one is robust on bigger ones)
\\
Implications: practical recommendations?
\\
Future work? (More diverse datasets, exploring additional strategies, applying in real-world tasks)

\section{Cool ideas}
\begin{itemize}
	\item Show the typical framework of active learning in a figure in the introduction
\end{itemize}

\section{Good formulations}
From the active learning survey: Entropy is an information-theoretic
measure that represents the amount of information needed to “encode” a distri-
bution. As such, it is often thought of as a measure of uncertainty or impurity in machine learning.

\bibliography{bibliography}
\bibliographystyle{icml2023}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
