{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sympy.printing.pretty.pretty_symbology import line_width\n",
    "from tueplots import figsizes, fontsizes, fonts, bundles, cycler\n",
    "from tueplots.constants.color import palettes\n",
    "from itertools import cycle\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "# Add the parent directory of `utils.py` to the Python path\n",
    "sys.path.append(str(Path('../code').resolve()))\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from utils import load_model_and_metrics, load_file, random_sampling, ranked_uc_and_dv_query"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following file contains Plots that are used in the project report.\n",
    "I use tueplots to generate plots with font sizes and sizes that automatically match the ones used in the paper.  "
   ],
   "id": "11192957ef74e7a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load data needed for plotting and set variables to make all the figures uniform",
   "id": "e8be00b403c34ec2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "line_width = 0.5\n",
    "line_alpha = 0.5"
   ],
   "id": "a5e8643763efb7e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "experiment = \"1\"\n",
    "dataset_name = \"MNIST\"\n",
    "query_methods = [\n",
    "    \"entropy_sampling\",\n",
    "    \"margin_sampling\",\n",
    "    \"random_sampling\",\n",
    "    # \"ranked_batch_mode\",\n",
    "    # \"ranked_uc_and_dv_0_5\",\n",
    "    \"uncertainty_sampling\",\n",
    "    'vote_entropy_committee'\n",
    "]\n",
    "clear_names = {\n",
    "    \"entropy_sampling\": \"Entropy Sampling\",\n",
    "    \"margin_sampling\": \"Margin Sampling\",\n",
    "    \"random_sampling\": \"Random Sampling\",\n",
    "    # \"ranked_batch_mode\": \"Ranked Batch Mode\",\n",
    "    # \"ranked_uc_and_dv_0_5\": \"RankedUcDv\",\n",
    "    \"uncertainty_sampling\": \"Uncertainty Sampling\",\n",
    "    'vote_entropy': 'Vote Entropy'\n",
    "}\n",
    "query_colors = {}\n",
    "\n",
    "exp_dir = os.path.join('../results', dataset_name, f'exp{experiment}')\n",
    "models_dir = os.path.join(exp_dir, 'models')\n",
    "metrics_dir = os.path.join(exp_dir, 'metrics')\n",
    "figs_dir = os.path.join('../doc', 'fig')"
   ],
   "id": "a7863312f14670b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_list = {}\n",
    "for file in os.listdir(models_dir):\n",
    "    query_method = file[:-10] # remove \"_model.pkl\" to retrieve the query method\n",
    "    model, metrics = load_model_and_metrics(experiment, dataset_name, query_method, base_path=exp_dir)\n",
    "    models_list[query_method] = (model, metrics)"
   ],
   "id": "2211d2e6b3f8567d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "datasets = load_file(os.path.join(exp_dir, 'datasets.pkl'))",
   "id": "80420e9152fbb870",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_losses = {}\n",
    "test_losses = {}\n",
    "test_accuracies = {}\n",
    "for query_method, (_, metrics) in models_list.items():\n",
    "    train_losses[query_method] = metrics['train_loss']\n",
    "    test_losses[query_method] = metrics['test_loss']\n",
    "    test_accuracies[query_method] = metrics['test_acc']\n",
    "train_losses_initial, test_losses_initial, test_accuracies_initial = train_losses.pop('initial_active_model'), test_losses.pop('initial_active_model'), test_accuracies.pop('initial_active_model')\n",
    "train_losses_whole_dataset, test_losses_whole_dataset, test_accuracies_whole_dataset = train_losses.pop('whole_dataset'), test_losses.pop('whole_dataset'), test_accuracies.pop('whole_dataset')"
   ],
   "id": "38a5974c268891b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_test, y_test = datasets['X_test'], datasets['y_test']\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "y_hats_probs = {}\n",
    "y_hats_classes = {}\n",
    "for query_method, (model, _) in models_list.items():\n",
    "    y_hats_probs[query_method] = model.predict_proba(X_test)\n",
    "    y_hats_classes[query_method] = model.predict(X_test)"
   ],
   "id": "bcc81db32d405451",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Helper methods\n",
    "These are not used here but if further experiments are done and early stopping used, then the dictionary lengths has to match for plots. These methods handle that"
   ],
   "id": "d87b571611798455"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Interpolation methods, in case some vectors are shorter than others. E.g. When doing active learning experiments, every data point corresponds to a single added sample, with additional fine-tuning. But in Pool-based methods, if n_instances=5, a data point corresponds to 5 added samples and fine-tuning performance.\n",
    "\n",
    "def duplicate_points(data, factor):\n",
    "    return [value for value in data for _ in range(factor)]\n",
    "\n",
    "def align_data_lengths(data, factor):\n",
    "    x_original = np.linspace(0, 1, len(data))\n",
    "    x_target = np.linspace(0, 1, len(data) * factor)\n",
    "    interpolator = interp1d(x_original, data, kind=\"linear\")\n",
    "    return interpolator(x_target)"
   ],
   "id": "ab1a479ee301047f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Plots\n",
    "\n",
    "---"
   ],
   "id": "c451f7e7c92a2980"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_data(data:dict, x_ax_label:str, y_ax_label:str, title:str, whole_dataset_baseline:list, show_legend:bool=True, interpolation_method=align_data_lengths, file_name=None, column_size=\"half\"):\n",
    "    color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "    \n",
    "    plt.rcParams.update(bundles.icml2022(column=column_size, nrows=1, ncols=1))\n",
    "    plt.rcParams.update({\"figure.dpi\": 350})\n",
    "    if column_size == \"full\":\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "        figsize[1] = figsize[1]/2\n",
    "        plt.rcParams.update({'figure.figsize': figsize})\n",
    "    plt.rcParams.update(cycler.cycler(color=palettes.tue_plot))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    max_iterations = max([len(l) for i, l in data.items()]) # In case, the lengths of the input vectors do not match.\n",
    "    r_b_u = data.pop('ranked_batch_mode')\n",
    "    for method, d in data.items():\n",
    "        is_committee = \"committee\" in method\n",
    "        query_method = method.replace(\"_committee\", \"\") # Extract query method (without duplicates)\n",
    "        if query_method not in query_colors:\n",
    "            query_colors[query_method] = next(color_cycle)\n",
    "        \n",
    "        # Check if interpolation is needed in case of different sized vectors\n",
    "        num_queries = len(d)\n",
    "        if num_queries < max_iterations:\n",
    "            factor = int(max_iterations/num_queries)\n",
    "            d = interpolation_method(d, factor)\n",
    "        \n",
    "        linestyle = \"-\" if is_committee else \"--\"\n",
    "        plt.plot([x*5 for x in range(1, max_iterations + 1)], \n",
    "                 d, \n",
    "                 label=clear_names[query_method] if not is_committee or query_method=='vote_entropy' else None,\n",
    "                 linewidth=line_width, \n",
    "                 linestyle=linestyle if not query_method=='vote_entropy' else '--',\n",
    "                 color=query_colors[query_method], \n",
    "                 alpha=line_alpha)\n",
    "    # Plot ranked batch mode separately as only the single model was trained\n",
    "    if 'ranked_batch_mode' not in query_colors:\n",
    "            query_colors['ranked_batch_mode'] = next(color_cycle)\n",
    "    plt.plot([x*5 for x in range(1, max_iterations + 1)],\n",
    "             r_b_u,\n",
    "             label='Ranked Batch-mode',\n",
    "             linewidth=line_width, \n",
    "             linestyle='--',\n",
    "             color=query_colors['ranked_batch_mode'], \n",
    "             alpha=line_alpha)\n",
    "    \n",
    "    # plt.axhline(y=whole_dataset_baseline[-1], color='black', linestyle='-', label='whole_dataset', linewidth=line_width) # Theoretical maximum (Model with the whole MNIST Train Dataset)\n",
    "    # Add labels, legend, and title\n",
    "    plt.xlabel(x_ax_label)\n",
    "    plt.ylabel(y_ax_label)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    \n",
    "    if show_legend:\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        unique_labels = dict(zip(labels, handles))\n",
    "        # Add single model and committee example lines\n",
    "        single_model_line = plt.Line2D([0], [0], color='black', linestyle='--', linewidth=line_width, label='Committee')\n",
    "        committee_line = plt.Line2D([0], [0], color='black', linestyle='-', linewidth=line_width, label='Single Model')\n",
    "        handles[-1].set_linestyle('-') # Set linestyle of ranked batch mode to - for uniform design\n",
    "        plt.legend(\n",
    "            handles=list(unique_labels.values()) + [single_model_line, committee_line],\n",
    "            loc='upper center', bbox_to_anchor=(0.5, 1.3), ncol=3\n",
    "        )\n",
    "        handles[-1].set_linestyle('--') # Set linestyle of Ranked batch mode bach to --\n",
    "    plt.grid(alpha=0.1)\n",
    "    if file_name is not None:\n",
    "        plt.savefig(os.path.join(figs_dir, file_name), dpi=350)\n",
    "    plt.show()"
   ],
   "id": "b09378def139279e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train loss and Test Loss",
   "id": "fdfccd812ba36dff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_data(data=train_losses.copy(), \n",
    "          x_ax_label=\"Number of added examples\", \n",
    "          y_ax_label=\"Train Loss (whole dataset)\", \n",
    "          title=None, \n",
    "          whole_dataset_baseline=train_losses_whole_dataset,\n",
    "          show_legend=False, \n",
    "          file_name=\"train_loss.pdf\")"
   ],
   "id": "a63a2af93ff3b346",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_data(data=test_losses.copy(), \n",
    "          x_ax_label=\"Queried Samples\", \n",
    "          y_ax_label=\"Test Loss\", \n",
    "          title=None, \n",
    "          whole_dataset_baseline=test_losses_whole_dataset,\n",
    "          show_legend=False, \n",
    "          file_name='test_loss.pdf')"
   ],
   "id": "d749c15dcc87d341",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test Accuracy",
   "id": "da652ad5b0d99025"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_data(data=test_accuracies.copy(), \n",
    "          x_ax_label=\"Queried Samples\", \n",
    "          y_ax_label=\"Test Accuracy\", \n",
    "          title=None, \n",
    "          whole_dataset_baseline=test_accuracies_whole_dataset, \n",
    "          column_size=\"full\", \n",
    "          show_legend=True,\n",
    "          file_name='test_accuracy.pdf')"
   ],
   "id": "f7a0542b18b1fb99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ROC Curve (One-vs-Rest)\n",
    "\n",
    "Due to the fact that a ROC curve is more for a binary task as it plots the TPR vs FPR, and we have multiple classes, I opt out to use the One vs Rest approach. (Another option would be One vs One, where each class is compared to each other class but due to the fact that there are 10 classes in MNIST, this approach is not taken)"
   ],
   "id": "2fed0b2d7d5130b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "line_width = 1",
   "id": "b94d97a21f37fc4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2d279aa55dd28b9",
   "metadata": {},
   "source": [
    "for query_method, _ in models_list.items():\n",
    "    plt.rcParams.update(bundles.icml2022(column=\"half\", nrows=1, ncols=1))\n",
    "    plt.rcParams.update({\"figure.dpi\": 350})\n",
    "    plt.rcParams.update(cycler.cycler(color=palettes.tue_plot))\n",
    "    \n",
    "    y_hat_model = y_hats_probs[query_method]\n",
    "    for c in classes:\n",
    "        y_true_c = np.where(y_test == c, 1, 0)\n",
    "        y_hat_c = y_hat_model[:, c]\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_true_c, y_hat_c)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "        plt.plot(fpr, tpr, label=f'{c}: (AUC = {roc_auc:.2f})', linewidth=line_width, alpha=0.3)\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', linewidth=line_width)  # this is the diagonal baseline\n",
    "    \n",
    "    plt.title(f'ROC Curves for {query_method}')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(alpha=line_alpha)\n",
    "    \n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5569459333d7a602",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "id": "4ee7efef8089b90b",
   "metadata": {},
   "source": [
    "for query_method in query_methods:\n",
    "    plt.rcParams.update(bundles.icml2022(column=\"full\", nrows=1, ncols=1))\n",
    "    plt.rcParams.update({\"figure.dpi\": 350})\n",
    "     # manually set the font size inside the confusion matrix\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, gridspec_kw={'width_ratios': [1, 1]}, constrained_layout=True)\n",
    "    # fig.suptitle(clear_names[query_method])  # Use readable names\n",
    "    fig.text(0.5, 0.85, clear_names[query_method], ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    y_hat_model = y_hats_classes[query_method]\n",
    "    y_hat_committee = y_hats_classes[f\"{query_method}_committee\"]\n",
    "\n",
    "    cm_model = confusion_matrix(y_test, y_hat_model)\n",
    "    cm_committee = confusion_matrix(y_test, y_hat_committee)\n",
    "    # Normalize\n",
    "    cm_model = cm_model / cm_model.sum(axis=1, keepdims=True)\n",
    "    cm_committee = cm_committee / cm_committee.sum(axis=1, keepdims=True)\n",
    "    norm = mcolors.Normalize(vmin=0, vmax=1)\n",
    "    \n",
    "    # Model Confusion matrix\n",
    "    disp_model = ConfusionMatrixDisplay(confusion_matrix=cm_model, display_labels=classes)\n",
    "    disp_model.plot(cmap=\"Blues\", ax=axs[1], xticks_rotation='horizontal', colorbar=False, values_format=None)\n",
    "    axs[1].set_title('Committee', fontsize=10)\n",
    "    \n",
    "    # Committee Confusion matrix\n",
    "    disp_committee = ConfusionMatrixDisplay(confusion_matrix=cm_committee, display_labels=classes)\n",
    "    im = disp_committee.plot(cmap=\"Blues\", ax=axs[0], xticks_rotation='horizontal', colorbar=False, values_format=None)\n",
    "    axs[0].set_title('Single Model', fontsize=10)\n",
    "    \n",
    "    cbar = fig.colorbar(im.im_, ax=axs, location='right', shrink=0.63175, aspect=15, pad=0.05, norm=norm)\n",
    "    cbar.set_label(\"Frequency\")\n",
    "    # Overwrite values in matrix to have 2 digits\n",
    "    for num in axs[0].texts:\n",
    "        num.remove()\n",
    "    for num in axs[1].texts:\n",
    "        num.remove()\n",
    "    for i in range(cm_model.shape[0]):\n",
    "        for j in range(cm_model.shape[1]):\n",
    "            axs[0].text(j, i, f\"{cm_model[i, j]:.2f}\", \n",
    "                        ha=\"center\", va=\"center\", color=\"black\" if i!=j else \"white\",\n",
    "                        fontsize=8)\n",
    "\n",
    "    for i in range(cm_committee.shape[0]):\n",
    "        for j in range(cm_committee.shape[1]):\n",
    "            axs[1].text(j, i, f\"{cm_committee[i, j]:.2f}\", \n",
    "                        ha=\"center\", va=\"center\", color=\"black\" if i!=j else \"white\",\n",
    "                        fontsize=8)\n",
    "    \n",
    "    plt.savefig(os.path.join(figs_dir, f'conf_mat_{query_method}.pdf'), dpi=350)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "93ca514d1b93d0b3",
   "metadata": {},
   "source": [
    "## Precision, Recall, F1-Score\n",
    "\n",
    "- Macro: Average accuracy equally across all classes. --> Metric is for each class independently\n",
    "- Micro: Account for class imbalance by aggregating globally. --> Imbalance-aware\n",
    "- Weighted Average: Weights class metrics by number of true samples in each classes support. Classes with more samples have larger influence."
   ]
  },
  {
   "cell_type": "code",
   "id": "7962c2230af60002",
   "metadata": {},
   "source": [
    "for query_method, y_preds in y_hats_classes.items():\n",
    "    print(f\"{query_method}: \\n\"\n",
    "          f\"{classification_report(y_test, y_preds, target_names=[str(c) for c in classes])}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for method, accs in test_accuracies.items():\n",
    "    is_committee = \"committee\" in method\n",
    "    query_method = method.replace(\"_committee\", \"\")\n",
    "    print(f'{clear_names[query_method]} {\"committee\" if is_committee else \"single model\"}: {accs[-1]}')"
   ],
   "id": "9d89a50b6a584a7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_accuracies_whole_dataset[-1]",
   "id": "6a5920f11ff95526",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
