{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Common Variables and Imports",
   "id": "2f51c4a3d74244ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T18:31:54.673800Z",
     "start_time": "2025-01-10T18:31:54.662655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, time, warnings, pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, log_loss, pairwise_distances\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling, entropy_sampling, margin_sampling, classifier_uncertainty, classifier_entropy, classifier_margin\n",
    "from modAL.batch import uncertainty_batch_sampling\n",
    "from modAL.expected_error import expected_error_reduction\n",
    "from modAL.density import information_density\n",
    "from torch import nn\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import load_CIFAR, log_metrics, initialize_random_number_generators, create_cnn_model, save_model_and_metrics, load_model_and_metrics, save_file\n",
    "\n",
    "# Filter FutureWarnings to make outputs look more pleasant and ConvergenceWarnings which are given by sklearn LogisticRegressors when explicitly settings the multi_class to multinomial. Here, this could be omitted but I liked to leave it in for clarity to show that I'm not training 10 binary classifiers but one classifier with 10 outputs, each resembling the probabilities of a digit\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ],
   "id": "bf40dce6e55cb97a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T18:31:55.240767Z",
     "start_time": "2025-01-10T18:31:55.214335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RANDOM_SEED = 42\n",
    "epochs = 50\n",
    "model_parameters={\n",
    "    'max_iterations_per_epoch': 20,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'input_size': 32, # This is the size of the images (32 x 32) which is used to calculate the size of the input for the NN after Convolutional layers\n",
    "    'num_channels': 3,\n",
    "    'l1_channels': 32,\n",
    "    'l1_kernel_size': 3,  # 3x3 convolution\n",
    "    'l2_channels': 64,\n",
    "    'l2_kernel_size': 3,\n",
    "    'l2_max_pool_kernel_size': 2,  # 2x2 max pooling\n",
    "    'l2_dropout': 0.25,\n",
    "    'l3_dropout': 0.5,\n",
    "    'l4_input': 1024,\n",
    "    'l4_dropout': 0.25,\n",
    "    'l5_input': 2048,\n",
    "    'output_size': 10  # CIFAR-10 has 10 classes\n",
    "}\n",
    "\n",
    "initialize_random_number_generators(RANDOM_SEED)\n",
    "\n",
    "experiment = \"2\" # \"1\" or \"2\" or \"3\"\n",
    "dataset_name = \"CIFAR\""
   ],
   "id": "a8442883d6651568",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T18:31:56.007250Z",
     "start_time": "2025-01-10T18:31:55.996859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "experiment_parameters = {\"1\": {\"n_initial\" : 10},\n",
    "                         \"2\": {\"n_initial\" : 2500}, # TODO: UPDATE THIS\n",
    "                         \"3\": {\"n_initial\" : 10000}}\n",
    "\n",
    "n_query_instances = 250\n",
    "n_initial = experiment_parameters[experiment][\"n_initial\"]"
   ],
   "id": "de18d1472564317f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Load Dataset (CIFAR)\n",
    "\n",
    "The datasets are saved in the experiment folders for convenience and checking whether the splits, etc. are actually the same."
   ],
   "id": "319858033c6a6d13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T18:32:07.178463Z",
     "start_time": "2025-01-10T18:31:56.999717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, y_train, X_test, y_test, X_whole, y_whole = load_CIFAR()\n",
    "class_names = ['Airplane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "save_file(os.path.join(\"../results\", dataset_name,  f\"exp{experiment}\", \"datasets.pkl\"), {\"X_train\": X_train, \"y_train\": y_train, \"X_test\": X_test, \"y_test\": y_test})"
   ],
   "id": "8981b010982ba151",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check how often each class appears in CIFAR and whether the data for each class is balanced",
   "id": "f25b1ed32e6ccbf3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T18:32:07.191180Z",
     "start_time": "2025-01-10T18:32:07.182008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(10):\n",
    "    print(f\"Class {i} ({class_names[i]}): {np.count_nonzero(y_whole == i)} times\")"
   ],
   "id": "1d07dcd32f95410",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 (Airplane): 6000 times\n",
      "Class 1 (Car): 6000 times\n",
      "Class 2 (Bird): 6000 times\n",
      "Class 3 (Cat): 6000 times\n",
      "Class 4 (Deer): 6000 times\n",
      "Class 5 (Dog): 6000 times\n",
      "Class 6 (Frog): 6000 times\n",
      "Class 7 (Horse): 6000 times\n",
      "Class 8 (Ship): 6000 times\n",
      "Class 9 (Truck): 6000 times\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Helper methods\n",
    "\n",
    "These are basically the same as in the shallow_classifier but due to the fact that Torch is used to train the NN, I need to use torch to remove the queried instances"
   ],
   "id": "59dc5c4062507364"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T18:32:07.219138Z",
     "start_time": "2025-01-10T18:32:07.191180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\" \n",
    "    Method for pool-based active learning query strategies. \n",
    "\"\"\"\n",
    "def train_active_learner(model_params, query_strat, n_query_instances:int, epochs:int, random_seed:int, pool_idx:list[int], X_initial, y_initial, create_model):\n",
    "    initialize_random_number_generators(seed=random_seed)\n",
    "    \n",
    "    X_pool = X_train[pool_idx]\n",
    "    y_pool = y_train[pool_idx]\n",
    "    \n",
    "    model = create_model(model_params, random_seed=RANDOM_SEED)\n",
    "    \n",
    "    learner = ActiveLearner(estimator=model, \n",
    "                            query_strategy=query_strat, \n",
    "                            X_training=X_initial, \n",
    "                            y_training=y_initial)\n",
    "    \n",
    "    # Passing X_training and y_training to the ActiveLearner automatically calls the fit method for log_reg with these data points\n",
    "\n",
    "    metrics = {'queries': [], 'train_loss': [], 'train_loss_current': [], 'test_loss': [], 'test_acc': []}\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(epochs): # epochs=n_queries to have both models trained on the same number of overall epochs\n",
    "        query_idx, query_inst = learner.query(X_pool, n_instances=n_query_instances)\n",
    "        \n",
    "        # Simulate labeling\n",
    "        learner.teach(X_pool[query_idx], y_pool[query_idx], only_new=False)\n",
    "        \n",
    "        # Remove queried point(s)\n",
    "        X_pool, y_pool = np.delete(X_pool, query_idx, axis=0), np.delete(y_pool, query_idx, axis=0)\n",
    "    \n",
    "        metrics['queries'].append(query_idx)\n",
    "        # log_metrics logs train loss for the whole train dataset which doesn't reflect the actual value in the current step but gives the ability to compare both models on the training set. \n",
    "        # To log training state on the actual (current) training set, do this additionally\n",
    "        is_cnn = (dataset_name == \"CIFAR\")\n",
    "        if is_cnn:\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            learner.estimator.module_.eval()  # Put the model into evaluation mode\n",
    "\n",
    "            with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "                X_training_tensor = torch.tensor(learner.X_training, dtype=torch.float32, device=device)\n",
    "                curr_train_logits = learner.estimator.forward(X_training_tensor)\n",
    "                curr_train_y_tensor = torch.tensor(learner.y_training, dtype=torch.long, device=device)\n",
    "                train_loss_current = criterion(curr_train_logits,\n",
    "                                               curr_train_y_tensor).item()\n",
    "    \n",
    "            learner.estimator.module_.train()  # Restore training mode\n",
    "            \n",
    "        else:\n",
    "            train_loss_current = log_loss(learner.y_training, learner.predict_proba(learner.X_training))\n",
    "        metrics['train_loss_current'].append(train_loss_current)\n",
    "        \n",
    "        log_metrics((epoch+1)*model_params[\"max_iterations_per_epoch\"], learner, X_train, y_train, X_test, y_test, metrics, is_cnn=is_cnn, device=device)\n",
    "        print(f\"       Current train loss: {train_loss_current:.4f}\")\n",
    "    print(f\"Training time: {time.time()-start:.2f} seconds\")\n",
    "    \n",
    "    return learner.estimator, metrics\n",
    "\n",
    "\"\"\" \n",
    "    Method for stream-based active learning query strategies. \n",
    "    \n",
    "    arguments:\n",
    "    - model_params: dict\n",
    "    - query_strat: dict\n",
    "    - query_score_threshold: float\n",
    "    - epochs: int\n",
    "    - random_seed: int\n",
    "    - X_stream: np.ndarray, typically full dataset\n",
    "    - y_stream: np.ndarray, typically full dataset\n",
    "    - X_initial: np.ndarray, initial training points\n",
    "    - y_initial: np.ndarray, initial training points\n",
    "\"\"\"\n",
    "def train_active_learner_stream(model_params, query_score_fn, n_query_instances:int, query_score_threshold:float, epochs:int, random_seed:int, X_stream, y_stream, X_initial, y_initial, create_model):    \n",
    "    initialize_random_number_generators(seed=random_seed)\n",
    "    \n",
    "    model = create_model(model_params, random_seed=RANDOM_SEED)\n",
    "    \n",
    "    learner = ActiveLearner(estimator=model, \n",
    "                            X_training=X_initial, \n",
    "                            y_training=y_initial)\n",
    "    # Passing X_training and y_training to the ActiveLearner automatically calls the fit method for log_reg with these data points\n",
    "\n",
    "    metrics = {'queries': [], 'train_loss': [], 'train_loss_current': [], 'test_loss': [], 'test_acc': []}\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # In pool based training, I get n_query_instances in each epoch. To have a comparable amount of data points for retraining the classifier, I use max_instances which equates to the amount of instances, a model in pool based approaches has seen. \n",
    "    max_instances = n_query_instances * epochs \n",
    "    used_instances = 0\n",
    "    \n",
    "    # Prevent infinite looping in case that no sample fulfills the query condition\n",
    "    retry_count, max_retries = 0, 10000\n",
    "    \n",
    "    # Use a random permutation of the whole MNIST train data set to mimic a data stream. \n",
    "    stream_indices, stream_pointer = np.random.permutation(len(X_stream)), 0\n",
    "    \n",
    "    while used_instances < max_instances:\n",
    "        if stream_pointer >= len(stream_indices):  # Restart stream simulation if the loop went through the whole list of data points\n",
    "            stream_indices = np.random.permutation(len(X_stream))\n",
    "            stream_pointer = 0\n",
    "        \n",
    "        stream_idx = stream_indices[stream_pointer]\n",
    "        x_instance, y_instance = X_stream[stream_idx].reshape(1, -1), y_stream[stream_idx].reshape(-1,)\n",
    "        stream_pointer += 1\n",
    "        retry_count += 1\n",
    "        \n",
    "        query_score = query_score_fn(learner, x_instance)\n",
    "        \n",
    "        # Depending on the function, we want to select samples with either a high score (uncertainty) or a low score (margin)\n",
    "        \n",
    "        if query_score_fn == classifier_margin:\n",
    "            query_condition = query_score < query_score_threshold\n",
    "        else: # classifier_uncertainty, classifier_entropy\n",
    "            query_condition = query_score > query_score_threshold\n",
    "        \n",
    "        if query_condition:\n",
    "            learner.teach(x_instance, y_instance)\n",
    "    \n",
    "            metrics['queries'].append(stream_idx)\n",
    "            # log_metrics logs train loss for the whole train dataset which doesn't reflect the actual value in the current step but gives the ability to compare both models on the training set. \n",
    "            # To log training state on the actual (current) training set, do this additionally\n",
    "            train_loss_current = log_loss(learner.y_training, learner.predict_proba(learner.X_training))\n",
    "            metrics['train_loss_current'].append(train_loss_current)\n",
    "            \n",
    "            log_metrics((used_instances+1), learner, X_train, y_train, X_test, y_test, metrics)\n",
    "            print(f\"       Current train loss: {train_loss_current:.4f}\")\n",
    "            \n",
    "            used_instances += 1\n",
    "            retry_count = 0\n",
    "        \n",
    "        if retry_count > max_retries:\n",
    "            print(f\"No suitable example could be found after {max_retries} retries, so training is stopped early.\")\n",
    "            break\n",
    "            \n",
    "    print(f\"Training time: {time.time()-start:.2f} seconds\")\n",
    "    \n",
    "    return learner.estimator, metrics"
   ],
   "id": "9bc50d7231c8089a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating an initial labelled dataset from random datapoints and the unlabelled pool\n",
    "\n",
    "If n_initial is smaller than 20, the model cannot be initialized properly and will throw errors so exactly one sample from each class is picked from a random permutation as the initial training set. "
   ],
   "id": "e5e2e186d810f76e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T18:32:07.266026Z",
     "start_time": "2025-01-10T18:32:07.219138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if n_initial == 10:\n",
    "    initial_idx = []\n",
    "    for cls in np.arange(10):\n",
    "        cls_idxs = np.where(y_train == cls)[0]\n",
    "        initial_idx.append(np.random.choice(cls_idxs))\n",
    "    # construct the X and y initial with one item from each class from a random permutation. the initial idx should keep the original mnist index.\n",
    "    # This is done to ensure that the model has an initial train set where it has seen each class\n",
    "    # Sadly, otherwise it will throw errors\n",
    "else:\n",
    "    initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False) # Indices with which the initial train set is created with\n",
    "\n",
    "X_initial = X_train[initial_idx]\n",
    "y_initial = y_train[initial_idx]\n",
    "pool_idx = np.setdiff1d(range(len(X_train)), initial_idx)"
   ],
   "id": "e6f0eebb42b902b6",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Train a Logistic Regressor on the whole train dataset\n",
    "Use multi_class='multinomial' solver: if it were 'ovr', each of the 10 output neurons would treat the corresponding number as a one-vs-rest szenario. So we would construct a Binary Distribution for each of the output neurons. But this doesn't take care of interdependencies between classes. \n",
    "In General: 'ovr' would train a separate classifier for each number and 'multinomial' does a softmax regression\n",
    "\n",
    "I could use ovr to have more control about the individual classes and to save computational costs \n",
    "\n",
    "Other option would be lbfgs which is also compatible with L2 and multiclass"
   ],
   "id": "ff515952001c52c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_full_model():\n",
    "    log_reg_full = create_cnn_model(model_parameters, random_seed=RANDOM_SEED)\n",
    "    \n",
    "    metrics = {'train_loss': [], 'test_loss': [], 'test_acc': []}\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        log_reg_full.fit(X_train, y_train)\n",
    "        log_metrics((epoch+1) * model_parameters[\"max_iterations_per_epoch\"], log_reg_full, X_train, y_train, X_test, y_test, metrics)\n",
    "\n",
    "    y_hat = log_reg_full.predict(X_test)\n",
    "    accuracy_whole_dataset = accuracy_score(y_test, y_hat)\n",
    "    print(f\"Test accuracy with whole Test dataset: {accuracy_whole_dataset:.4f}\")\n",
    "    print(f\"Training time: {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    return log_reg_full, metrics\n",
    "\n",
    "log_reg_whole_data, log_reg_whole_data_metrics = train_full_model()\n",
    "save_model_and_metrics(experiment, dataset_name, \"whole_dataset\", log_reg_whole_data, log_reg_whole_data_metrics)"
   ],
   "id": "30d1698ed021029a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Active Learning\n",
    "\n",
    "---"
   ],
   "id": "ca339ac4a579fa91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train a Logistic Regressor on 100 data points without Active Learning\n",
    "\n",
    "This serves as a baseline to show how much can be learnt from n_initial data points. (This is expected to be low)"
   ],
   "id": "5029f71bc2e61288"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_initial_model():\n",
    "    initialize_random_number_generators(seed=RANDOM_SEED)\n",
    "    \n",
    "    log_reg_initial = create_cnn_model(model_parameters, random_seed=RANDOM_SEED)\n",
    "    \n",
    "    metrics = {'train_loss': [], 'train_loss_current': [], 'test_loss': [], 'test_acc': []}\n",
    "    start = time.time()\n",
    "    if len(X_initial) > 0:\n",
    "        log_reg_initial.fit(X_initial, y_initial)\n",
    "        print(f\"Train time: {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    y_pred = log_reg_initial.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "     \n",
    "    train_loss_current = log_loss(y_initial, log_reg_initial.predict_proba(X_initial))\n",
    "    metrics['train_loss_current'].append(train_loss_current)\n",
    "    \n",
    "    log_metrics(model_parameters[\"max_iterations_per_epoch\"], log_reg_initial, X_train, y_train, X_test, y_test, metrics)\n",
    "\n",
    "    print(f\"Test accuracy with whole Test dataset: {accuracy:.4f}\")\n",
    "    \n",
    "    return log_reg_initial, metrics\n",
    "\n",
    "log_reg_initial, log_reg_initial_metrics = train_initial_model()\n",
    "save_model_and_metrics(experiment, dataset_name, \"initial_active_model\", log_reg_initial, log_reg_initial_metrics)"
   ],
   "id": "f070945732a153f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train a Classifier with Various Query Strategies",
   "id": "b0711f048f244356"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From the documents and maybe worth trying: If you would like to start from scratch, you can use the .fit(X, y) method to make the learner forget everything it has seen and fit the model to the newly provided data.\n",
    "\n",
    "To train only on the newly acquired data, you should pass only_new=True to the .teach() method. "
   ],
   "id": "28e693b8576a43c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Random Sampling",
   "id": "e0d16dd6c5dba075"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T18:56:25.573124Z",
     "start_time": "2025-01-10T18:32:14.000504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def random_sampling(classifier, X_pool, n_instances):\n",
    "    n_samples = len(X_pool)\n",
    "    query_idx = np.random.choice(range(n_samples), size=n_instances, replace=False)\n",
    "    return query_idx, X_pool[query_idx]\n",
    "\n",
    "learner, metrics = train_active_learner(model_params=model_parameters, query_strat=random_sampling, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial, create_model=create_cnn_model)\n",
    "save_model_and_metrics(experiment, dataset_name, \"random_sampling\", learner, metrics)"
   ],
   "id": "c0f2252e8a37778e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20: - Train Loss: 2.1442 - Test Loss: 2.2404 - Test Accuracy: 0.4909\n",
      "       Current train loss: 0.0352\n",
      "Iteration 40: - Train Loss: 2.3493 - Test Loss: 2.4520 - Test Accuracy: 0.5097\n",
      "       Current train loss: 0.0139\n",
      "Iteration 60: - Train Loss: 2.3144 - Test Loss: 2.4494 - Test Accuracy: 0.5027\n",
      "       Current train loss: 0.0026\n",
      "Iteration 80: - Train Loss: 2.2192 - Test Loss: 2.3784 - Test Accuracy: 0.5142\n",
      "       Current train loss: 0.0052\n",
      "Iteration 100: - Train Loss: 1.9960 - Test Loss: 2.1271 - Test Accuracy: 0.5239\n",
      "       Current train loss: 0.0035\n",
      "Iteration 120: - Train Loss: 2.0401 - Test Loss: 2.1821 - Test Accuracy: 0.5396\n",
      "       Current train loss: 0.0016\n",
      "Iteration 140: - Train Loss: 2.0088 - Test Loss: 2.1634 - Test Accuracy: 0.5401\n",
      "       Current train loss: 0.0027\n",
      "Iteration 160: - Train Loss: 1.9023 - Test Loss: 2.0546 - Test Accuracy: 0.5552\n",
      "       Current train loss: 0.0016\n",
      "Iteration 180: - Train Loss: 2.3466 - Test Loss: 2.5202 - Test Accuracy: 0.5424\n",
      "       Current train loss: 0.1385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-10T18:56:28.750372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def random_sampling(classifier, X_pool, n_instances):\n",
    "    n_samples = len(X_pool)\n",
    "    query_idx = np.random.choice(range(n_samples), size=n_instances, replace=False)\n",
    "    return query_idx, X_pool[query_idx]\n",
    "\n",
    "learner, metrics = train_active_learner(model_params=model_parameters, query_strat=random_sampling, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial, create_model=create_cnn_model)\n",
    "save_model_and_metrics(experiment, dataset_name, \"random_sampling\", learner, metrics)"
   ],
   "id": "cebf0a8d9be9c6c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Uncertainty sampling strategies",
   "id": "ffb2eb9af020267d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Uncertainty Sampling**: Samples where classifier is least sure are selected",
   "id": "36cf1785e4959be6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner(model_params=model_parameters, query_strat=uncertainty_sampling, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, dataset_name, \"uncertainty_sampling\", learner, metrics)"
   ],
   "id": "10fbdcdade2f050",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Entropy Sampling**: Samples where class probability has the largest Entropy",
   "id": "b2433d3fb21e7422"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner(model_params=model_parameters, query_strat=entropy_sampling, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, dataset_name, \"uncertainty_entropy_sampling\", learner, metrics)"
   ],
   "id": "bb318d73efbbd6a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Margin Sampling**: Selects instances where difference between first most likely and second most likely classes are the smallest",
   "id": "31e16b8e2de5c8cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner(model_params=model_parameters, query_strat=margin_sampling, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, dataset_name, \"uncertainty_margin_sampling\", learner, metrics)"
   ],
   "id": "842e3ccea720b828",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "afffb41bb387ae43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ranked Batch-Mode Sampling\n",
    "\n",
    "$$score=\\alpha\\left(1-\\Phi\\left(x, X_{\\text {labeled }}\\right)\\right)+(1-\\alpha) U(x)$$\n",
    "where $\\alpha=\\frac{\\left|X_{\\text {unlabeled }}\\right|}{\\left|X_{\\text {unlabeled }}\\right|+\\left|X_{\\text {labeled }}\\right|}, X_{\\text {labeled }}$ is the labeled dataset, $U(x)$ is the uncertainty of predictions for $x$, and $\\Phi$ is a so-called similarity function, for instance cosine similarity. This latter function measures how well the feature space is explored near $x$. (The lower the better.)\n",
    "\n",
    "According to the modAL docs: This strategy differs from uncertainty_sampling() because, although it is supported, traditional active learning query strategies suffer from sub-optimal record selection when passing n_instances > 1. This sampling strategy extends the interactive uncertainty query sampling by allowing for batch-mode uncertainty query sampling. Furthermore, it also enforces a ranking – that is, which records among the batch are most important for labeling?"
   ],
   "id": "fe351425d5454487"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner(model_params=model_parameters, query_strat=uncertainty_batch_sampling, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, dataset_name, \"ranked_batch_mode\", learner, metrics)"
   ],
   "id": "c29ca5f79c9fd0e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Custom combination between uncertainty of the data point(s) in X and diversity between X and parts of the train dataset",
   "id": "ce39ffdbd8b31fc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "alpha_uc_dv = 0.5\n",
    "def ranked_uc_and_dv_score(learner, X):\n",
    "    uncertainty = classifier_uncertainty(learner, X)\n",
    "    diversity = np.min(pairwise_distances(X, learner.X_training), axis=1)\n",
    "    combined_scores = alpha_uc_dv * uncertainty + (1 - alpha_uc_dv) * diversity\n",
    "    return combined_scores\n",
    "\n",
    "def ranked_uc_and_dv_query(learner, X, n_instances=1):\n",
    "    uc_dv_scores = ranked_uc_and_dv_score(learner, X)\n",
    "    # Sort them in descending order\n",
    "    ranked_indices = np.argsort(uc_dv_scores)[::-1]\n",
    "    selected_indices = ranked_indices[:n_instances]\n",
    "    selected_instances = X[selected_indices]\n",
    "    \n",
    "    return selected_indices, selected_instances\n",
    "\n",
    "for a, a_str in [(0.2, \"0_2\"), (0.5, \"0_5\"), (0.8, \"0_8\")]:\n",
    "    alpha_uc_dv = a\n",
    "    learner, metrics = train_active_learner(model_params=model_parameters, query_strat=ranked_uc_and_dv_query, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "    save_model_and_metrics(experiment, dataset_name, f\"ranked_uc_and_dv_{a_str}\", learner, metrics)"
   ],
   "id": "81e38f74bc551cca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Stream-Based sampling",
   "id": "9313d3b94cbcc787"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "with Uncertainty Sampling/ Classifier uncertainty as it's query score method",
   "id": "23207125e2c1d2fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for uncertainty_threshold, uncertainty_threshold_str in [(0.2, \"0_2\"), (0.5, \"0_5\"), (0.8, \"0_8\")]:\n",
    "    learner, metrics = train_active_learner_stream(model_params=model_parameters, query_score_fn=classifier_uncertainty, query_score_threshold=uncertainty_threshold, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, X_stream=X_train, y_stream=y_train, X_initial=X_initial, y_initial=y_initial)\n",
    "    save_model_and_metrics(experiment, dataset_name, f\"stream_classifier_uncertainty_th_{uncertainty_threshold_str}\", learner, metrics)"
   ],
   "id": "f6f5e127aa6f12f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "with Classification margin uncertainty as it's query score method",
   "id": "c25eff2e9e5241c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner_stream(model_params=model_parameters, query_score_fn=classifier_margin, query_score_threshold=0.5, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, X_stream=X_train, y_stream=y_train, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, dataset_name, \"stream_classifier_margin\", learner, metrics)"
   ],
   "id": "76a83a3c69453ec5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "with Entropy margin uncertainty as it's query score method",
   "id": "1b8491fcd52e92a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner_stream(model_params=model_parameters, query_score_fn=classifier_entropy, query_score_threshold=0.5, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, X_stream=X_train, y_stream=y_train, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, dataset_name, \"stream_entropy\", learner, metrics)"
   ],
   "id": "68c1a37d0246eb96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "with the custom measurement of uncertainty and diversity of the already seen datapoints",
   "id": "c1168161a537716"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for uncertainty_threshold, uncertainty_threshold_str in [(0.2, \"0_2\"), (0.5, \"0_5\"), (0.8, \"0_8\")]:\n",
    "    for a, a_str in [(0.2, \"0_2\"), (0.5, \"0_5\"), (0.8, \"0_8\")]:\n",
    "        alpha_uc_dv = a\n",
    "        learner, metrics = train_active_learner_stream(\n",
    "            model_params=model_parameters, \n",
    "            query_score_fn=ranked_uc_and_dv_score, \n",
    "            query_score_threshold=uncertainty_threshold, \n",
    "            n_query_instances=n_query_instances, \n",
    "            epochs=epochs, random_seed=RANDOM_SEED, \n",
    "            X_stream=X_train, \n",
    "            y_stream=y_train, \n",
    "            X_initial=X_initial, \n",
    "            y_initial=y_initial)\n",
    "        save_model_and_metrics(experiment, dataset_name, \n",
    "                               f\"stream_classifier_ranked_uc_and_dv_{a_str}_th_{uncertainty_threshold_str}\", \n",
    "                               learner, metrics)"
   ],
   "id": "d0b1004d79d9f7f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Disagreement Sampling (for classifiers) (uses a committee, so I should theoretically do every train run again for each methodwith a committee!)",
   "id": "4229e26196ee7f82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5f60ad2552bc169d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Methods that sadly don't work \n",
    "\n",
    "---"
   ],
   "id": "6e0181a457ba1a9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Expected error reduction (doesn't work on my pc due to time complexity)",
   "id": "64cc96ce3b0ea18b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# learner, metrics = train_active_learner(model_params=model_parameters, query_strat=expected_error_reduction, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)",
   "id": "cdc036fb6be26710",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Information Density (doesn't work on my pc due to time complexity)\n",
    "\n",
    "$$I(x)=\\frac{1}{\\left|X_u\\right|} \\sum_{x^{\\prime} \\in X} \\operatorname{sim}\\left(x, x^{\\prime}\\right)$$\n",
    "\n",
    "where $\\operatorname{sim}\\left(x, x^{\\prime}\\right)$ is a similarity function such as cosine similarity or Euclidean similarity, which is the reciprocal of Euclidean distance. The higher the information density, the more similar the given instance is to the rest of the data.\n",
    "\n",
    "\n",
    "According to the modAL docs: When using uncertainty sampling (or other similar strategies), we are unable to take the structure of the data into account which can lead to suboptimal queries.\n",
    "\n",
    "This could very well be used in combination with another strategy"
   ],
   "id": "724ccd53be1e214f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def inf_density(classifier, X_pool):\n",
    "#     return information_density(X_pool, metric='euclidean')\n",
    "# \n",
    "# learner, metrics = train_active_learner(model_params=model_parameters, query_strat=inf_density,  epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)"
   ],
   "id": "c75a65b997a22566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "This is for using it with a Committee (for multiple classes) so it's not optimal for comparing the query strategies themselves maybe?"
   ],
   "id": "ea2db7db79104728"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "Acquisition Functions might not be usable due to the fact that they require a BayesianOptimizer, not an ActiveLearner"
   ],
   "id": "f2cbbd3fbf405c5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Acquisition Functions",
   "id": "f872cf1438ff1a5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Probability of improvement**: \n",
    "$$PI(x)=\\psi\\left(\\frac{\\mu(x) - f\\left(x^+\\right) - \\xi}{\\sigma(x)}\\right)$$\n",
    "where $\\mu(x)$ and $\\sigma(x)$ are mean and variance of regressor at $x$, $f$ is the model to be optimized with estimated maximum at $x^+$. $\\xi$ is a parameter controlling the degree of exploration and $\\psi(x)$ denotes cumulative distribution function of a standard Gaussian Distribution\n",
    "\n",
    "[Example from the ModAL Docs](https://modal-python.readthedocs.io/en/latest/_images/bo-PI.png)\n"
   ],
   "id": "ef26dec36c690061"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# learner, metrics = train_active_learner(model_params=model_parameters, query_strat=max_PI, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)",
   "id": "b77668772d478e5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**[Expected Improvement (from the ModAL Docs)](https://modal-python.readthedocs.io/en/latest/content/query_strategies/Acquisition-functions.html#expected-improvement)**: \n",
    "$$\n",
    "EI(x) = \n",
    "\\left( \\mu(x) - f(x^+) - \\xi \\right) \\cdot \\psi \\left( \\frac{\\mu(x) - f(x^+) - \\xi}{\\sigma(x)} \\right)\n",
    "+ \\sigma(x) \\phi \\left( \\frac{\\mu(x) - f(x^+) - \\xi}{\\sigma(x)} \\right),\n",
    "$$\n",
    "\n",
    "where $\\mu(x)$ and $\\sigma(x)$ are the mean and variance of the regressor at $x$, $f$ is the function to be optimized with estimated maximum at $x$, $\\xi$ is a parameter controlling the degree of exploration, and $\\psi(z), \\phi(z)$ denote the cumulative distribution function and density function of a standard Gaussian distribution."
   ],
   "id": "571c93bd34cf6a53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Upper Confidence Bound**:\n",
    "$$ UCB(x) = \\mu(x) + \\beta \\sigma(x)$$\n",
    "where $\\mu(x)$ and $\\sigma(x)$ are mean and variance of the regressor and $\\beta$ is a parameter controlling the degree of exploration\n",
    "\n",
    "[Example from the ModAL Docs](https://modal-python.readthedocs.io/en/latest/_images/bo-UCB.png)"
   ],
   "id": "e8ebdeda7bf2d25e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
