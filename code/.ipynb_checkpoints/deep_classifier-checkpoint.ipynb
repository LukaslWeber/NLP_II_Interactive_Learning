{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f51c4a3d74244ee",
   "metadata": {},
   "source": [
    "# Common Variables and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf40dce6e55cb97a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T23:02:23.482679Z",
     "start_time": "2025-01-24T23:01:59.588883Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, time, warnings, pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, log_loss, pairwise_distances\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling, entropy_sampling, margin_sampling, classifier_uncertainty, classifier_entropy, classifier_margin\n",
    "from modAL.batch import uncertainty_batch_sampling\n",
    "#from modAL.expected_error import expected_error_reduction\n",
    "from modAL.density import information_density\n",
    "from torch import nn\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import train_active_learner, load_CIFAR, log_metrics, initialize_random_number_generators, create_cnn_model, save_model_and_metrics, load_model_and_metrics, save_file, load_file, compute_loss, train_committee_learner\n",
    "\n",
    "# Filter FutureWarnings to make outputs look more pleasant and ConvergenceWarnings which are given by sklearn LogisticRegressors when explicitly settings the multi_class to multinomial. Here, this could be omitted but I liked to leave it in for clarity to show that I'm not training 10 binary classifiers but one classifier with 10 outputs, each resembling the probabilities of a digit\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8442883d6651568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T23:02:23.511583Z",
     "start_time": "2025-01-24T23:02:23.482679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device; cpu\n",
      "{'input_size': 32, 'num_channels': 3, 'l1_channels': 64, 'l1_kernel_size': 5, 'l1_padding': 1, 'l1_stride': 1, 'l2_channels': 64, 'l2_kernel_size': 5, 'l2_max_pool_kernel_size': 5, 'l2_padding': 1, 'l2_stride': 1, 'l2_dropout': 0.2, 'l3_dropout': 0.3, 'l4_input': 2048, 'l4_dropout': 0.3, 'l5_input': 1024, 'output_size': 10, 'lr': 0.0005, 'weight_decay': 0.0001, 'max_epochs': 175, 'batch_size': 256}\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"using device; {device}\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "initialize_random_number_generators(RANDOM_SEED)\n",
    "\n",
    "# This can be used to specify the experiment, e.g. when trying to test something new\n",
    "# Previously it was used to test the 3 experimental settings (differing sizes of the initial dataset) with the following dictionary:\n",
    "# experiment_parameters = {\"1\": {\"n_initial\" : 10},\n",
    "#                          \"2\": {\"n_initial\" : 2500},\n",
    "#                          \"3\": {\"n_initial\" : 10000}}\n",
    "experiment = \"1\" # \"1\" or \"2\" or \"3\"\n",
    "dataset_name = \"CIFAR\"\n",
    "\n",
    "model_parameters=load_file(\"deep_classifier_parameters.pkl\") #TODO\n",
    "print(model_parameters)\n",
    "\n",
    "# To use early stopping, do the models have to be trained for every single epoch\n",
    "max_iterations = model_parameters['max_epochs']\n",
    "patience = 50 # Controls early stopping iterations without improvement\n",
    "\n",
    "# Active Learning parameters\n",
    "n_query_instances = 250 # Amount of instances that are queried at a time\n",
    "n_initial = 5000 # Initial set of labeled datapoints\n",
    "n_query_epochs = 50  # How many times the algorithm should sample n_query_instances samples\n",
    "n_iter_active_learning = 20 # During active learning, train for n_iter_active_learning epochs before querying a new sample. This controls overfitting\n",
    "\n",
    "# For Committee based approaches\n",
    "n_learners=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319858033c6a6d13",
   "metadata": {},
   "source": [
    "# Load Dataset (CIFAR)\n",
    "\n",
    "The datasets are saved in the experiment folders for convenience and checking whether the splits, etc. are actually the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8981b010982ba151",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T23:02:31.122324Z",
     "start_time": "2025-01-24T23:02:23.512592Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, X_val, y_val, X_whole, y_whole = load_CIFAR(random_seed=RANDOM_SEED)\n",
    "class_names = ['Airplane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "save_file(os.path.join(\"../results\", dataset_name,  f\"exp{experiment}\", \"datasets.pkl\"), {\"X_train\": X_train, \"y_train\": y_train, \"X_test\": X_test, \"y_test\": y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b1ed32e6ccbf3",
   "metadata": {},
   "source": [
    "Check how often each class appears in CIFAR and whether the data for each class is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d07dcd32f95410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T22:57:53.994543Z",
     "start_time": "2025-01-24T22:57:53.982610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 (Airplane): 6000 times\n",
      "Class 1 (Car): 6000 times\n",
      "Class 2 (Bird): 6000 times\n",
      "Class 3 (Cat): 6000 times\n",
      "Class 4 (Deer): 6000 times\n",
      "Class 5 (Dog): 6000 times\n",
      "Class 6 (Frog): 6000 times\n",
      "Class 7 (Horse): 6000 times\n",
      "Class 8 (Ship): 6000 times\n",
      "Class 9 (Truck): 6000 times\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Class {i} ({class_names[i]}): {np.count_nonzero(y_whole == i)} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2e186d810f76e",
   "metadata": {},
   "source": [
    "## Creating an initial labelled dataset from random datapoints and the unlabelled pool\n",
    "\n",
    "If n_initial is smaller than 10, the model cannot be initialized properly and will throw errors so exactly one sample from each class is picked from a random permutation as the initial training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f0eebb42b902b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T23:02:31.154426Z",
     "start_time": "2025-01-24T23:02:31.122324Z"
    }
   },
   "outputs": [],
   "source": [
    "if n_initial == 10:\n",
    "    initial_idx = []\n",
    "    for cls in np.arange(10):\n",
    "        cls_idxs = np.where(y_train == cls)[0]\n",
    "        initial_idx.append(np.random.choice(cls_idxs))\n",
    "    # construct the X and y initial with one item from each class from a random permutation. the initial idx should keep the original mnist index.\n",
    "    # This is done to ensure that the model has an initial train set where it has seen each class\n",
    "    # Sadly, otherwise it will throw errors\n",
    "else:\n",
    "    initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False) # Indices with which the initial train set is created with\n",
    "\n",
    "X_initial = X_train[initial_idx]\n",
    "y_initial = y_train[initial_idx]\n",
    "pool_idx = np.setdiff1d(range(len(X_train)), initial_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c086a2247d7ca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T23:02:31.167423Z",
     "start_time": "2025-01-24T23:02:31.157934Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets = {'dataset_name': dataset_name,\n",
    "            'X_initial': X_initial,\n",
    "            'y_initial': y_initial,\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val, \n",
    "            'pool_idx': pool_idx}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff515952001c52c3",
   "metadata": {},
   "source": [
    "# Train a CNN on the whole train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d1698ed021029a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-21T07:03:30.073464Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_full_model():\n",
    "    best_loss = np.inf\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    model_parameters['max_epochs'] = 1\n",
    "    cnn_full = create_cnn_model(model_parameters, random_seed=RANDOM_SEED)\n",
    "    model_parameters['max_epochs'] = max_iterations\n",
    "    \n",
    "    \n",
    "    metrics = {'train_loss': [], 'test_loss': [], 'test_acc': []}\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(max_iterations):\n",
    "        cnn_full.fit(X_train, y_train)\n",
    "        if epoch % 10 == 0:\n",
    "            log_metrics(epoch, cnn_full, X_train, y_train, X_test, y_test, metrics, is_cnn=True, device=device)\n",
    "    \n",
    "        val_loss = compute_loss(y_hat=cnn_full.predict_proba(X_val), y_data=y_val)\n",
    "        \n",
    "        # cnn_full.module_.eval()\n",
    "        # with torch.no_grad():\n",
    "        #     curr_val_logits = cnn_full.forward(torch.tensor(X_val, dtype=torch.float32, device=device))\n",
    "        #     val_loss = criterion(curr_val_logits, torch.tensor(y_val, dtype=torch.long, device=device)).item()\n",
    "        # cnn_full.module_.train()\n",
    "\n",
    "        \n",
    "        #if val_loss < best_loss:\n",
    "        #    best_loss = val_loss\n",
    "        #    no_improvement_count = 0\n",
    "        #else:\n",
    "        #    no_improvement_count += 1\n",
    "\n",
    "        #print(f\"  - Validation loss: {val_loss}\")\n",
    "        \n",
    "        #if no_improvement_count >= patience:\n",
    "        #    print(\"Early stopping triggered.\")\n",
    "        #    break\n",
    "\n",
    "    y_hat = cnn_full.predict(X_test)\n",
    "    accuracy_whole_dataset = accuracy_score(y_test, y_hat)\n",
    "    print(f\"Test accuracy with whole Test dataset: {accuracy_whole_dataset:.4f}\")\n",
    "    print(f\"Training time: {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    return cnn_full, metrics\n",
    "\n",
    "cnn_whole_data, cnn_whole_data_metrics = train_full_model()\n",
    "save_model_and_metrics(experiment, dataset_name, \"whole_dataset\", cnn_whole_data, cnn_whole_data_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca339ac4a579fa91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Active Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029f71bc2e61288",
   "metadata": {},
   "source": [
    "### Train a Logistic Regressor on 100 data points without Active Learning\n",
    "\n",
    "This serves as a baseline to show how much can be learnt from n_initial data points. (This is expected to be low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f070945732a153f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T22:46:26.118869Z",
     "start_time": "2025-01-24T22:46:02.305905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m3.5469\u001b[0m  19.8929\n",
      "      2        \u001b[36m1.8523\u001b[0m  20.0257\n",
      "      3        \u001b[36m1.6955\u001b[0m  19.5617\n",
      "      4        \u001b[36m1.5947\u001b[0m  19.2454\n",
      "      5        \u001b[36m1.5082\u001b[0m  19.5146\n",
      "      6        \u001b[36m1.4655\u001b[0m  19.5757\n",
      "      7        \u001b[36m1.3806\u001b[0m  19.8260\n",
      "      8        \u001b[36m1.3042\u001b[0m  19.9215\n",
      "      9        \u001b[36m1.2727\u001b[0m  19.9210\n",
      "     10        \u001b[36m1.2193\u001b[0m  19.7156\n",
      "     11        \u001b[36m1.1593\u001b[0m  19.6916\n",
      "     12        \u001b[36m1.0614\u001b[0m  19.7436\n",
      "     13        \u001b[36m1.0153\u001b[0m  19.8118\n",
      "     14        \u001b[36m0.9205\u001b[0m  20.0606\n",
      "     15        \u001b[36m0.8629\u001b[0m  20.4369\n",
      "     16        \u001b[36m0.8274\u001b[0m  19.8797\n",
      "     17        \u001b[36m0.7219\u001b[0m  19.9239\n",
      "     18        \u001b[36m0.6826\u001b[0m  19.6705\n",
      "     19        \u001b[36m0.6284\u001b[0m  20.1231\n",
      "     20        \u001b[36m0.5475\u001b[0m  20.1871\n",
      "     21        \u001b[36m0.5085\u001b[0m  20.0481\n",
      "     22        \u001b[36m0.4517\u001b[0m  20.4860\n",
      "     23        \u001b[36m0.3982\u001b[0m  19.8593\n",
      "     24        \u001b[36m0.3327\u001b[0m  19.8551\n",
      "     25        \u001b[36m0.3156\u001b[0m  19.8139\n",
      "     26        \u001b[36m0.2877\u001b[0m  19.8916\n",
      "     27        \u001b[36m0.2837\u001b[0m  20.2045\n",
      "     28        \u001b[36m0.2483\u001b[0m  20.1877\n",
      "     29        0.2653  20.1259\n",
      "     30        \u001b[36m0.2176\u001b[0m  19.9852\n",
      "     31        \u001b[36m0.1894\u001b[0m  19.7815\n",
      "     32        0.1902  19.9017\n",
      "     33        0.2031  19.8952\n",
      "     34        \u001b[36m0.1775\u001b[0m  20.2149\n",
      "     35        \u001b[36m0.1504\u001b[0m  20.1746\n",
      "     36        \u001b[36m0.1326\u001b[0m  20.0760\n",
      "     37        0.1330  19.7885\n",
      "     38        0.1547  19.5573\n",
      "     39        0.1401  19.7802\n",
      "     40        0.1411  19.7659\n",
      "     41        \u001b[36m0.1183\u001b[0m  19.8438\n",
      "     42        \u001b[36m0.1021\u001b[0m  20.1865\n",
      "     43        \u001b[36m0.0984\u001b[0m  19.8873\n",
      "     44        \u001b[36m0.0971\u001b[0m  19.8443\n",
      "     45        0.1142  19.8144\n",
      "     46        0.1032  19.9052\n",
      "     47        \u001b[36m0.0895\u001b[0m  19.9652\n",
      "     48        \u001b[36m0.0778\u001b[0m  20.2804\n",
      "     49        0.0860  20.0340\n",
      "     50        \u001b[36m0.0759\u001b[0m  19.8745\n",
      "     51        \u001b[36m0.0738\u001b[0m  19.6395\n",
      "     52        0.0759  19.7612\n",
      "     53        0.0797  19.6540\n",
      "     54        0.0752  19.8273\n",
      "     55        \u001b[36m0.0722\u001b[0m  20.0680\n",
      "     56        \u001b[36m0.0653\u001b[0m  20.0385\n",
      "     57        0.0702  19.9064\n",
      "     58        0.0726  19.6747\n",
      "     59        0.0734  19.8500\n",
      "     60        0.0748  19.9079\n",
      "     61        \u001b[36m0.0588\u001b[0m  19.9832\n",
      "     62        \u001b[36m0.0557\u001b[0m  19.9220\n",
      "     63        0.0643  20.0044\n",
      "     64        \u001b[36m0.0543\u001b[0m  20.0585\n",
      "     65        \u001b[36m0.0453\u001b[0m  19.7925\n",
      "     66        0.0689  19.6055\n",
      "     67        0.0493  19.9697\n",
      "     68        0.0512  19.8414\n",
      "     69        0.0495  20.1719\n",
      "     70        0.0511  19.9881\n",
      "     71        \u001b[36m0.0423\u001b[0m  19.9231\n",
      "     72        0.0440  19.8275\n",
      "     73        0.0490  19.7175\n",
      "     74        0.0428  19.9676\n",
      "     75        \u001b[36m0.0405\u001b[0m  20.0004\n",
      "     76        0.0469  20.0428\n",
      "     77        0.0432  20.1878\n",
      "     78        0.0422  19.8445\n",
      "     79        0.0435  19.8090\n",
      "     80        0.0472  20.0804\n",
      "     81        0.0433  20.4552\n",
      "     82        0.0483  20.9571\n",
      "     83        0.0417  21.3074\n",
      "     84        \u001b[36m0.0365\u001b[0m  22.0099\n",
      "     85        0.0451  21.7578\n",
      "     86        \u001b[36m0.0358\u001b[0m  22.1632\n",
      "     87        \u001b[36m0.0274\u001b[0m  22.3373\n",
      "     88        0.0344  22.5730\n",
      "     89        0.0377  22.7241\n",
      "     90        \u001b[36m0.0263\u001b[0m  22.9754\n",
      "     91        0.0280  23.6370\n",
      "     92        0.0365  23.8958\n",
      "     93        0.0291  23.7040\n",
      "     94        0.0320  24.3885\n",
      "     95        0.0376  24.2703\n",
      "     96        0.0364  24.5595\n",
      "     97        0.0265  25.1036\n",
      "     98        0.0346  25.0492\n",
      "     99        0.0294  26.1656\n",
      "    100        0.0390  26.1338\n",
      "    101        0.0331  26.0884\n",
      "    102        0.0279  26.2878\n",
      "    103        \u001b[36m0.0187\u001b[0m  26.0695\n",
      "    104        0.0311  26.8899\n",
      "    105        0.0213  27.2766\n",
      "    106        0.0289  27.8801\n",
      "    107        0.0360  28.2557\n",
      "    108        0.0407  27.9591\n",
      "    109        0.0238  28.5495\n",
      "    110        0.0250  30.3133\n",
      "    111        0.0232  29.1002\n",
      "    112        0.0272  27.8959\n",
      "    113        0.0305  28.2560\n",
      "    114        0.0302  29.6800\n",
      "    115        0.0304  30.2631\n",
      "    116        0.0318  30.0268\n",
      "    117        0.0284  30.8748\n",
      "    118        0.0254  30.7636\n",
      "    119        0.0195  31.3162\n",
      "    120        0.0262  32.1036\n",
      "    121        0.0280  32.4822\n",
      "    122        0.0255  33.0889\n",
      "    123        0.0288  33.2132\n",
      "    124        0.0260  32.1701\n",
      "    125        0.0294  32.2413\n",
      "    126        0.0246  32.2382\n",
      "    127        0.0207  33.3547\n",
      "    128        0.0235  34.8154\n",
      "    129        0.0297  34.4080\n",
      "    130        0.0293  33.7649\n",
      "    131        0.0228  35.6482\n",
      "    132        0.0216  36.3276\n",
      "    133        0.0257  36.5387\n",
      "    134        0.0254  35.7389\n",
      "    135        0.0294  37.9348\n",
      "    136        0.0255  39.2587\n",
      "    137        0.0200  37.6614\n",
      "    138        0.0196  38.3475\n",
      "    139        0.0283  40.0514\n",
      "    140        0.0251  40.6361\n",
      "    141        0.0223  40.5913\n",
      "    142        0.0246  40.3387\n",
      "    143        0.0258  37.2788\n",
      "    144        0.0212  36.6863\n",
      "    145        0.0218  36.7188\n",
      "    146        \u001b[36m0.0154\u001b[0m  36.8533\n",
      "    147        0.0179  37.6439\n",
      "    148        0.0215  37.8787\n",
      "    149        0.0172  38.0426\n",
      "    150        0.0258  37.5256\n",
      "    151        0.0213  39.3139\n",
      "    152        0.0187  40.4036\n",
      "    153        0.0255  39.0507\n",
      "    154        0.0251  40.5002\n",
      "    155        0.0201  40.6826\n",
      "    156        \u001b[36m0.0147\u001b[0m  41.1106\n",
      "    157        0.0153  41.4833\n",
      "    158        \u001b[36m0.0139\u001b[0m  41.4244\n",
      "    159        0.0148  42.6905\n",
      "    160        0.0232  43.3189\n",
      "    161        0.0206  44.5967\n",
      "    162        0.0235  43.9031\n",
      "    163        0.0222  43.6967\n",
      "    164        0.0231  45.7495\n",
      "    165        0.0210  43.8557\n",
      "    166        \u001b[36m0.0133\u001b[0m  45.4375\n",
      "    167        0.0176  47.4955\n",
      "    168        0.0166  46.5667\n",
      "    169        0.0133  46.5022\n",
      "    170        0.0152  46.9030\n",
      "    171        0.0166  47.0531\n",
      "    172        0.0134  48.4434\n",
      "    173        0.0179  49.4544\n",
      "    174        0.0153  48.6261\n",
      "    175        0.0154  48.5328\n",
      "Train time: 4833.47 seconds\n",
      "After iteration 175: \n",
      "  - Train Loss: 2.6063 \n",
      "  - Test Loss: 2.3801 \n",
      "  - Test Accuracy: 0.563\n",
      "Test accuracy with whole Test dataset: 0.5630\n"
     ]
    }
   ],
   "source": [
    "def train_initial_model():\n",
    "    initialize_random_number_generators(seed=RANDOM_SEED)\n",
    "    \n",
    "    log_reg_initial = create_cnn_model(model_parameters, random_seed=RANDOM_SEED)\n",
    "    \n",
    "    metrics = {'train_loss': [], 'test_loss': [], 'test_acc': []}\n",
    "    start = time.time()\n",
    "  \n",
    "    log_reg_initial.fit(X_initial, y_initial)\n",
    "    print(f\"Train time: {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    y_pred = log_reg_initial.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    log_metrics(model_parameters[\"max_epochs\"], log_reg_initial, X_train, y_train, X_test, y_test, metrics)\n",
    "\n",
    "    print(f\"Test accuracy with whole Test dataset: {accuracy:.4f}\")\n",
    "    \n",
    "    return log_reg_initial, metrics\n",
    "\n",
    "log_reg_initial, log_reg_initial_metrics = train_initial_model()\n",
    "save_model_and_metrics(experiment, dataset_name, \"initial_active_model\", log_reg_initial, log_reg_initial_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0711f048f244356",
   "metadata": {},
   "source": [
    "## Train a Classifier with Various Query Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e693b8576a43c2",
   "metadata": {},
   "source": [
    "From the documents and maybe worth trying: If you would like to start from scratch, you can use the .fit(X, y) method to make the learner forget everything it has seen and fit the model to the newly provided data.\n",
    "\n",
    "To train only on the newly acquired data, you should pass only_new=True to the .teach() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7569bee945126732",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T23:02:31.189429Z",
     "start_time": "2025-01-24T23:02:31.177514Z"
    }
   },
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"datasets\": datasets,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"n_query_instances\": n_query_instances,\n",
    "    \"n_query_epochs\": n_query_epochs,\n",
    "    \"create_model\": create_cnn_model, \n",
    "    \"model_params\": model_parameters, \n",
    "    \"n_iter_active_learning\": n_iter_active_learning,\n",
    "    \"patience\": patience\n",
    "}\n",
    "training_config_committee = training_config.copy()\n",
    "training_config_committee['n_learners'] = n_learners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d16dd6c5dba075",
   "metadata": {},
   "source": [
    "## Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0f2252e8a37778e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T23:02:31.199693Z",
     "start_time": "2025-01-24T23:02:31.189429Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_sampling(classifier, X_pool, n_instances):\n",
    "    n_samples = len(X_pool)\n",
    "    query_idx = np.random.choice(range(n_samples), size=n_instances, replace=False)\n",
    "    return query_idx, X_pool[query_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79613d21a659774f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T23:05:29.514337Z",
     "start_time": "2025-01-24T23:02:31.199693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss      dur\n",
      "-------  ------------  -------\n",
      "      1        \u001b[36m3.5469\u001b[0m  20.4214\n",
      "      2        \u001b[36m1.8523\u001b[0m  19.5690\n",
      "      3        \u001b[36m1.6955\u001b[0m  19.3404\n",
      "      4        \u001b[36m1.5947\u001b[0m  19.7510\n",
      "      5        \u001b[36m1.5082\u001b[0m  19.7948\n",
      "      6        \u001b[36m1.4655\u001b[0m  19.8474\n",
      "      7        \u001b[36m1.3806\u001b[0m  19.9067\n",
      "      8        \u001b[36m1.3042\u001b[0m  19.4518\n",
      "      9        \u001b[36m1.2727\u001b[0m  19.6197\n",
      "     10        \u001b[36m1.2193\u001b[0m  19.5436\n",
      "     11        \u001b[36m1.1593\u001b[0m  19.4814\n",
      "     12        \u001b[36m1.0614\u001b[0m  19.8223\n",
      "     13        \u001b[36m1.0153\u001b[0m  19.8884\n",
      "     14        \u001b[36m0.9205\u001b[0m  19.8874\n",
      "     15        \u001b[36m0.8629\u001b[0m  19.8291\n",
      "     16        \u001b[36m0.8274\u001b[0m  19.6121\n",
      "     17        \u001b[36m0.7219\u001b[0m  19.5727\n",
      "     18        \u001b[36m0.6826\u001b[0m  19.7373\n",
      "     19        \u001b[36m0.6284\u001b[0m  19.8752\n",
      "     20        \u001b[36m0.5475\u001b[0m  19.9443\n",
      "     21        \u001b[36m0.5085\u001b[0m  19.9470\n",
      "     22        \u001b[36m0.4517\u001b[0m  19.7629\n",
      "     23        \u001b[36m0.3982\u001b[0m  20.2205\n",
      "     24        \u001b[36m0.3327\u001b[0m  19.7461\n",
      "     25        \u001b[36m0.3156\u001b[0m  19.6220\n",
      "     26        \u001b[36m0.2877\u001b[0m  19.8246\n",
      "     27        \u001b[36m0.2837\u001b[0m  19.9380\n",
      "     28        \u001b[36m0.2483\u001b[0m  20.3605\n",
      "     29        0.2653  20.0151\n",
      "     30        \u001b[36m0.2176\u001b[0m  19.7991\n",
      "     31        \u001b[36m0.1894\u001b[0m  19.8851\n",
      "     32        0.1902  19.5119\n",
      "     33        0.2031  19.6232\n",
      "     34        \u001b[36m0.1775\u001b[0m  19.8853\n",
      "     35        \u001b[36m0.1504\u001b[0m  20.1149\n",
      "     36        \u001b[36m0.1326\u001b[0m  19.9381\n",
      "     37        0.1330  19.6998\n",
      "     38        0.1547  19.9232\n",
      "     39        0.1401  19.5129\n",
      "     40        0.1411  19.4896\n",
      "     41        \u001b[36m0.1183\u001b[0m  20.3609\n",
      "     42        \u001b[36m0.1021\u001b[0m  20.0108\n",
      "     43        \u001b[36m0.0984\u001b[0m  20.0995\n",
      "     44        \u001b[36m0.0971\u001b[0m  20.0001\n",
      "     45        0.1142  19.6961\n",
      "     46        0.1032  19.7977\n",
      "     47        \u001b[36m0.0895\u001b[0m  19.8291\n",
      "     48        \u001b[36m0.0778\u001b[0m  19.8552\n",
      "     49        0.0860  19.8442\n",
      "     50        \u001b[36m0.0759\u001b[0m  20.0651\n",
      "     51        \u001b[36m0.0738\u001b[0m  20.1393\n",
      "     52        0.0759  19.8299\n",
      "     53        0.0797  19.9405\n",
      "     54        0.0752  19.8072\n",
      "     55        \u001b[36m0.0722\u001b[0m  19.9413\n",
      "     56        \u001b[36m0.0653\u001b[0m  19.7197\n",
      "     57        0.0702  19.9060\n",
      "     58        0.0726  19.9210\n",
      "     59        0.0734  19.9972\n",
      "     60        0.0748  20.0809\n",
      "     61        \u001b[36m0.0588\u001b[0m  19.9171\n",
      "     62        \u001b[36m0.0557\u001b[0m  19.7164\n",
      "     63        0.0643  19.8151\n",
      "     64        \u001b[36m0.0543\u001b[0m  19.6536\n",
      "     65        \u001b[36m0.0453\u001b[0m  20.1532\n",
      "     66        0.0689  20.1092\n",
      "     67        0.0493  19.9369\n",
      "     68        0.0512  19.9202\n",
      "     69        0.0495  19.6391\n",
      "     70        0.0511  19.8398\n",
      "     71        \u001b[36m0.0423\u001b[0m  19.7471\n",
      "     72        0.0440  19.9813\n",
      "     73        0.0490  20.2072\n",
      "     74        0.0428  20.1611\n",
      "     75        \u001b[36m0.0405\u001b[0m  19.8940\n",
      "     76        0.0469  19.9540\n",
      "     77        0.0432  19.7646\n",
      "     78        0.0422  20.2207\n",
      "     79        0.0435  19.9039\n",
      "     80        0.0472  20.1563\n",
      "     81        0.0433  20.6114\n",
      "     82        0.0483  20.7565\n",
      "     83        0.0417  20.9457\n",
      "     84        \u001b[36m0.0365\u001b[0m  21.7615\n",
      "     85        0.0451  21.7563\n",
      "     86        \u001b[36m0.0358\u001b[0m  22.4188\n",
      "     87        \u001b[36m0.0274\u001b[0m  22.2449\n",
      "     88        0.0344  22.7297\n",
      "     89        0.0377  22.9676\n",
      "     90        \u001b[36m0.0263\u001b[0m  22.8126\n",
      "     91        0.0280  24.6606\n",
      "     92        0.0365  23.4321\n",
      "     93        0.0291  23.7357\n",
      "     94        0.0320  24.0038\n",
      "     95        0.0376  24.1092\n",
      "     96        0.0364  24.1593\n",
      "     97        0.0265  24.9303\n",
      "     98        0.0346  24.8035\n",
      "     99        0.0294  26.1297\n",
      "    100        0.0390  26.0721\n",
      "    101        0.0331  26.2716\n",
      "    102        0.0279  26.2581\n",
      "    103        \u001b[36m0.0187\u001b[0m  26.0214\n",
      "    104        0.0311  27.0940\n",
      "    105        0.0213  27.1058\n",
      "    106        0.0289  28.1861\n",
      "    107        0.0360  27.9258\n",
      "    108        0.0407  27.9693\n",
      "    109        0.0238  28.1589\n",
      "    110        0.0250  30.0944\n",
      "    111        0.0232  29.3065\n",
      "    112        0.0272  27.9997\n",
      "    113        0.0305  28.2468\n",
      "    114        0.0302  29.1102\n",
      "    115        0.0304  30.0765\n",
      "    116        0.0318  30.2608\n",
      "    117        0.0284  31.5221\n",
      "    118        0.0254  30.6714\n",
      "    119        0.0195  30.8027\n",
      "    120        0.0262  32.5751\n",
      "    121        0.0280  33.0215\n",
      "    122        0.0255  33.1845\n",
      "    123        0.0288  32.3853\n",
      "    124        0.0260  32.0204\n",
      "    125        0.0294  32.7253\n",
      "    126        0.0246  32.5401\n",
      "    127        0.0207  33.2434\n",
      "    128        0.0235  33.9688\n",
      "    129        0.0297  34.6920\n",
      "    130        0.0293  34.6095\n",
      "    131        0.0228  35.8462\n",
      "    132        0.0216  36.0348\n",
      "    133        0.0257  37.2189\n",
      "    134        0.0254  36.0372\n",
      "    135        0.0294  37.3398\n",
      "    136        0.0255  38.3118\n",
      "    137        0.0200  38.1231\n",
      "    138        0.0196  39.0699\n",
      "    139        0.0283  39.2856\n",
      "    140        0.0251  39.4915\n",
      "    141        0.0223  41.1078\n",
      "    142        0.0246  40.2653\n",
      "    143        0.0258  36.5722\n",
      "    144        0.0212  36.5046\n",
      "    145        0.0218  37.7543\n",
      "    146        \u001b[36m0.0154\u001b[0m  36.7758\n",
      "    147        0.0179  37.1028\n",
      "    148        0.0215  37.6107\n",
      "    149        0.0172  38.9721\n",
      "    150        0.0258  37.3607\n",
      "    151        0.0213  38.8006\n",
      "    152        0.0187  40.2001\n",
      "    153        0.0255  39.7080\n",
      "    154        0.0251  39.9106\n",
      "    155        0.0201  39.7227\n",
      "    156        \u001b[36m0.0147\u001b[0m  41.8918\n",
      "    157        0.0153  41.7657\n",
      "    158        \u001b[36m0.0139\u001b[0m  40.7202\n",
      "    159        0.0148  42.6751\n",
      "    160        0.0232  47.1671\n",
      "    161        0.0206  44.1049\n",
      "    162        0.0235  43.6377\n",
      "    163        0.0222  44.3401\n",
      "    164        0.0231  45.4580\n",
      "    165        0.0210  43.6534\n",
      "    166        \u001b[36m0.0133\u001b[0m  46.1422\n",
      "    167        0.0176  47.4508\n",
      "    168        0.0166  46.1484\n",
      "    169        0.0133  47.0263\n",
      "    170        0.0152  46.7589\n",
      "    171        0.0166  46.2367\n",
      "    172        0.0134  48.8018\n",
      "    173        0.0179  48.6732\n",
      "    174        0.0153  48.1478\n",
      "    175        0.0154  49.4718\n",
      "175\n",
      "20\n",
      "    176        0.1502  52.1534\n",
      "    177        0.0822  51.2684\n",
      "    178        0.0583  53.3901\n",
      "    179        0.0406  54.0530\n",
      "    180        0.0261  54.8298\n",
      "    181        0.0348  58.2296\n",
      "    182        0.0283  56.4362\n",
      "    183        0.0284  54.5873\n",
      "    184        0.0229  56.5644\n",
      "    185        0.0271  56.8319\n",
      "    186        0.0246  59.1133\n",
      "    187        0.0218  58.4537\n",
      "    188        0.0180  59.5625\n",
      "    189        0.0254  59.3149\n",
      "    190        0.0214  58.1204\n",
      "    191        0.0296  59.4696\n",
      "    192        0.0284  59.8173\n",
      "    193        0.0197  60.6442\n",
      "    194        0.0177  61.1422\n",
      "    195        0.0156  60.8950\n",
      "After iteration 0: \n",
      "  - Train Loss: 1.8445 \n",
      "  - Test Loss: 1.8991 \n",
      "  - Test Accuracy: 0.5674\n",
      "  - number of train samples: 5250\n",
      "    196        0.1354  67.1722\n",
      "    197        0.0764  63.9645\n",
      "    198        0.0610  63.5346\n",
      "    199        0.0429  64.8104\n",
      "    200        0.0364  65.4329\n",
      "    201        0.0266  67.8871\n",
      "    202        0.0259  68.3816\n",
      "    203        0.0209  69.1599\n",
      "    204        0.0217  68.9452\n",
      "    205        0.0222  67.7770\n",
      "    206        0.0201  68.8307\n",
      "    207        0.0243  68.3348\n",
      "    208        0.0273  69.9534\n",
      "    209        0.0254  68.2427\n",
      "    210        0.0234  68.2117\n",
      "    211        0.0237  69.3915\n",
      "    212        0.0187  69.3133\n",
      "    213        0.0289  71.7403\n",
      "    214        0.0224  69.1096\n",
      "    215        0.0285  71.4249\n",
      "After iteration 1: \n",
      "  - Train Loss: 1.8313 \n",
      "  - Test Loss: 1.8869 \n",
      "  - Test Accuracy: 0.5807\n",
      "  - number of train samples: 5500\n",
      "    216        0.1290  76.4451\n",
      "    217        0.0700  73.9111\n",
      "    218        0.0539  75.8765\n",
      "    219        0.0353  78.2150\n",
      "    220        0.0296  78.5384\n",
      "    221        0.0226  80.2624\n",
      "    222        0.0259  79.3050\n",
      "    223        0.0227  79.9404\n",
      "    224        0.0283  78.7508\n",
      "    225        0.0254  79.6416\n",
      "    226        0.0263  81.2115\n",
      "    227        0.0328  78.3217\n",
      "    228        0.0195  81.1728\n",
      "    229        0.0173  79.5303\n",
      "    230        0.0247  81.1357\n",
      "    231        0.0253  82.4790\n",
      "    232        0.0259  80.3277\n",
      "    233        0.0217  82.0193\n",
      "    234        0.0182  80.1988\n",
      "    235        0.0171  83.3559\n",
      "After iteration 2: \n",
      "  - Train Loss: 1.8316 \n",
      "  - Test Loss: 1.8941 \n",
      "  - Test Accuracy: 0.5682\n",
      "  - number of train samples: 5750\n",
      "    236        0.1436  87.5354\n",
      "    237        0.0938  85.8285\n",
      "    238        0.0558  85.2403\n",
      "    239        0.0451  86.7661\n",
      "    240        0.0285  86.2002\n",
      "    241        0.0303  87.5067\n",
      "    242        0.0315  90.4916\n",
      "    243        0.0272  88.4838\n",
      "    244        0.0291  94.2440\n",
      "    245        0.0250  91.8601\n",
      "    246        0.0271  91.5711\n",
      "    247        0.0312  89.8941\n",
      "    248        0.0207  92.3839\n",
      "    249        0.0215  95.7179\n",
      "    250        0.0225  93.6548\n",
      "    251        0.0184  95.3261\n",
      "    252        0.0194  93.4730\n",
      "    253        0.0199  95.5879\n",
      "    254        0.0203  94.6856\n",
      "    255        0.0281  94.3534\n",
      "After iteration 3: \n",
      "  - Train Loss: 1.8267 \n",
      "  - Test Loss: 1.8922 \n",
      "  - Test Accuracy: 0.5714\n",
      "  - number of train samples: 6000\n",
      "    256        0.1240  98.5552\n",
      "    257        0.0851  97.4360\n",
      "    258        0.0531  96.9633\n",
      "    259        0.0405  98.4578\n",
      "    260        0.0324  98.0437\n",
      "    261        0.0334  101.8708\n",
      "    262        0.0181  102.5910\n",
      "    263        0.0210  106.1698\n",
      "    264        0.0242  106.5702\n",
      "    265        0.0259  110.6377\n",
      "    266        0.0278  109.1445\n",
      "    267        0.0243  110.0188\n",
      "    268        0.0209  116.1147\n",
      "    269        0.0235  116.5620\n",
      "    270        0.0166  115.9729\n",
      "    271        0.0208  117.8872\n",
      "    272        0.0218  116.2154\n",
      "    273        0.0206  124.6186\n",
      "    274        0.0206  120.3675\n",
      "    275        0.0222  123.6538\n",
      "After iteration 4: \n",
      "  - Train Loss: 1.8239 \n",
      "  - Test Loss: 1.8877 \n",
      "  - Test Accuracy: 0.5793\n",
      "  - number of train samples: 6250\n",
      "    276        0.1208  131.2737\n",
      "    277        0.0791  125.8752\n",
      "    278        0.0553  124.0715\n",
      "    279        0.0390  126.9985\n",
      "    280        0.0294  130.6111\n",
      "    281        0.0249  132.0722\n",
      "    282        0.0271  132.8818\n",
      "    283        0.0209  131.2784\n",
      "    284        0.0187  134.5853\n",
      "    285        0.0265  136.4165\n",
      "    286        0.0232  140.5904\n",
      "    287        0.0169  138.5896\n",
      "    288        0.0185  140.8873\n",
      "    289        0.0218  143.5021\n",
      "    290        0.0218  145.1928\n",
      "    291        0.0212  146.7682\n",
      "    292        0.0189  147.9797\n",
      "    293        0.0301  149.8194\n",
      "    294        0.0285  147.7403\n",
      "    295        0.0251  149.0033\n",
      "After iteration 5: \n",
      "  - Train Loss: 1.8186 \n",
      "  - Test Loss: 1.8854 \n",
      "  - Test Accuracy: 0.5811\n",
      "  - number of train samples: 6500\n",
      "    296        0.1160  158.8720\n",
      "    297        0.0769  158.1338\n",
      "    298        0.0475  159.9096\n",
      "    299        0.0536  157.9444\n",
      "    300        0.0315  155.4904\n",
      "    301        0.0224  158.0156\n",
      "    302        0.0315  162.6610\n",
      "    303        0.0260  160.8687\n",
      "    304        0.0197  163.3749\n",
      "    305        0.0302  167.0909\n",
      "    306        0.0173  165.5943\n",
      "    307        0.0158  166.8286\n",
      "    308        0.0212  167.6274\n",
      "    309        0.0210  166.6495\n",
      "    310        0.0217  166.9349\n",
      "    311        0.0259  166.8726\n",
      "    312        0.0186  168.3049\n",
      "    313        0.0177  171.3789\n",
      "    314        0.0187  172.2007\n",
      "    315        0.0226  170.8671\n",
      "After iteration 6: \n",
      "  - Train Loss: 1.8063 \n",
      "  - Test Loss: 1.8745 \n",
      "  - Test Accuracy: 0.5943\n",
      "  - number of train samples: 6750\n",
      "    316        0.1224  177.4873\n",
      "    317        0.0722  170.8500\n",
      "    318        0.0445  177.1135\n",
      "    319        0.0332  176.3300\n",
      "    320        0.0312  173.6164\n",
      "    321        0.0258  179.9838\n",
      "    322        0.0278  182.9568\n",
      "    323        0.0285  180.2254\n",
      "    324        0.0200  180.5864\n",
      "    325        0.0256  186.0925\n",
      "    326        0.0155  184.2472\n",
      "    327        0.0190  185.8921\n",
      "    328        0.0219  188.0703\n",
      "    329        0.0233  189.3747\n",
      "    330        0.0288  184.9274\n",
      "    331        0.0202  189.3137\n",
      "    332        0.0260  189.3349\n",
      "    333        0.0234  184.4399\n",
      "    334        0.0201  190.2944\n",
      "    335        0.0192  194.1933\n",
      "After iteration 7: \n",
      "  - Train Loss: 1.8013 \n",
      "  - Test Loss: 1.8657 \n",
      "  - Test Accuracy: 0.6025\n",
      "  - number of train samples: 7000\n",
      "    336        0.1008  199.3252\n",
      "    337        0.0569  194.2319\n",
      "    338        0.0398  195.4013\n",
      "    339        0.0297  196.2852\n",
      "    340        0.0249  201.1618\n",
      "    341        0.0265  202.0464\n",
      "    342        0.0261  208.4862\n",
      "    343        0.0261  215.6734\n",
      "    344        0.0280  219.0627\n",
      "    345        0.0216  218.6165\n",
      "    346        0.0234  215.5475\n",
      "    347        0.0226  216.8043\n",
      "    348        0.0196  221.0842\n",
      "    349        0.0170  232.2825\n",
      "    350        0.0238  218.6653\n",
      "    351        0.0220  215.4941\n",
      "    352        0.0199  208.6780\n",
      "    353        0.0185  210.1086\n",
      "    354        0.0217  210.9402\n",
      "    355        0.0219  218.4204\n",
      "After iteration 8: \n",
      "  - Train Loss: 1.7992 \n",
      "  - Test Loss: 1.8661 \n",
      "  - Test Accuracy: 0.602\n",
      "  - number of train samples: 7250\n",
      "    356        0.1030  217.4836\n",
      "    357        0.0671  217.9372\n",
      "    358        0.0485  213.3895\n",
      "    359        0.0428  215.3975\n",
      "    360        0.0362  217.7732\n",
      "    361        0.0300  226.1188\n",
      "    362        0.0229  228.3603\n",
      "    363        0.0271  219.2040\n",
      "    364        0.0251  224.0594\n",
      "    365        0.0236  224.7420\n",
      "    366        0.0268  227.0363\n",
      "    367        0.0242  228.5756\n",
      "    368        0.0257  228.9723\n",
      "    369        0.0273  230.3796\n",
      "    370        0.0155  218.4944\n",
      "    371        0.0160  225.9001\n",
      "    372        0.0208  225.4851\n",
      "    373        0.0227  225.2944\n",
      "    374        0.0201  225.0789\n",
      "    375        0.0200  226.9440\n",
      "After iteration 9: \n",
      "  - Train Loss: 1.7921 \n",
      "  - Test Loss: 1.8631 \n",
      "  - Test Accuracy: 0.602\n",
      "  - number of train samples: 7500\n",
      "    376        0.0957  232.4146\n",
      "    377        0.0528  234.6737\n",
      "    378        0.0439  225.3403\n",
      "    379        0.0431  234.3615\n",
      "    380        0.0268  234.8510\n",
      "    381        0.0210  234.4176\n",
      "    382        0.0249  239.9859\n",
      "    383        0.0268  244.1842\n",
      "    384        0.0194  267.0328\n"
     ]
    }
   ],
   "source": [
    "learner, metrics = train_active_learner(query_strat=random_sampling, **training_config)\n",
    "save_model_and_metrics(experiment, dataset_name, \"random_sampling\", learner, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf0a8d9be9c6c7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-24T23:05:32.655928Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "c_random_sampling, c_random_sampling_metrics = train_committee_learner(query_strat=random_sampling, **training_config_committee)\n",
    "save_model_and_metrics(experiment, dataset_name, \"random_sampling_committee\", c_random_sampling, c_random_sampling_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2eb9af020267d",
   "metadata": {},
   "source": [
    "## Uncertainty sampling strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf1785e4959be6",
   "metadata": {},
   "source": [
    "**Uncertainty Sampling**: Samples where classifier is least sure are selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fbdcdade2f050",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "learner, metrics = train_active_learner(query_strat=uncertainty_sampling, **training_config)\n",
    "save_model_and_metrics(experiment, dataset_name, \"uncertainty_sampling\", learner, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff62b97d4aa7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_uc_sampling, c_uc_sampling_metrics = train_committee_learner(query_strat=uncertainty_sampling, **training_config_committee)\n",
    "save_model_and_metrics(experiment, dataset_name, \"uncertainty_sampling_committee\", c_uc_sampling, c_uc_sampling_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2433d3fb21e7422",
   "metadata": {},
   "source": [
    "**Entropy Sampling**: Samples where class probability has the largest Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb318d73efbbd6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner, metrics = train_active_learner(query_strat=entropy_sampling, **training_config)\n",
    "save_model_and_metrics(experiment, dataset_name, \"entropy_sampling\", learner, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2505fa3dbc40c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_entropy_sampling, c_entropy_sampling_metrics = train_committee_learner(query_strat=entropy_sampling, **training_config_committee)\n",
    "save_model_and_metrics(experiment, dataset_name, \"entropy_sampling_committee\", c_entropy_sampling, c_entropy_sampling_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e16b8e2de5c8cd",
   "metadata": {},
   "source": [
    "**Margin Sampling**: Selects instances where difference between first most likely and second most likely classes are the smallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e3ccea720b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner, metrics = train_active_learner(query_strat=margin_sampling, **training_config)\n",
    "save_model_and_metrics(experiment, dataset_name, \"margin_sampling\", learner, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b78061025a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_margin_sampling, c_margin_sampling_metrics = train_committee_learner(query_strat=margin_sampling, **training_config_committee)\n",
    "save_model_and_metrics(experiment, dataset_name, \"margin_sampling_committee\", c_margin_sampling, c_margin_sampling_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe351425d5454487",
   "metadata": {},
   "source": [
    "## Ranked Batch-Mode Sampling\n",
    "\n",
    "$$score=\\alpha\\left(1-\\Phi\\left(x, X_{\\text {labeled }}\\right)\\right)+(1-\\alpha) U(x)$$\n",
    "where $\\alpha=\\frac{\\left|X_{\\text {unlabeled }}\\right|}{\\left|X_{\\text {unlabeled }}\\right|+\\left|X_{\\text {labeled }}\\right|}, X_{\\text {labeled }}$ is the labeled dataset, $U(x)$ is the uncertainty of predictions for $x$, and $\\Phi$ is a so-called similarity function, for instance cosine similarity. This latter function measures how well the feature space is explored near $x$. (The lower the better.)\n",
    "\n",
    "According to the modAL docs: This strategy differs from uncertainty_sampling() because, although it is supported, traditional active learning query strategies suffer from sub-optimal record selection when passing n_instances > 1. This sampling strategy extends the interactive uncertainty query sampling by allowing for batch-mode uncertainty query sampling. Furthermore, it also enforces a ranking – that is, which records among the batch are most important for labeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ca5f79c9fd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner, metrics = train_active_learner(query_strat=uncertainty_batch_sampling, **training_config)\n",
    "save_model_and_metrics(experiment, dataset_name, \"ranked_batch_mode\", learner, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4fea3c19c4931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ranked_batch_mode_sampling, c_ranked_batch_mode_sampling_metrics = train_committee_learner(query_strat=uncertainty_batch_sampling, **training_config_committee)\n",
    "save_model_and_metrics(experiment, dataset_name, \"ranked_batch_mode_committee\", c_ranked_batch_mode_sampling, c_ranked_batch_mode_sampling_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce39ffdbd8b31fc5",
   "metadata": {},
   "source": [
    "## Custom combination between uncertainty of the data point(s) in X and diversity between X and parts of the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e38f74bc551cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_uc_dv = 0.5\n",
    "def ranked_uc_and_dv_score(learner, X):\n",
    "    uncertainty = classifier_uncertainty(learner, X)\n",
    "    diversity = np.min(pairwise_distances(X, learner.X_training), axis=1)\n",
    "    combined_scores = alpha_uc_dv * uncertainty + (1 - alpha_uc_dv) * diversity\n",
    "    return combined_scores\n",
    "\n",
    "def ranked_uc_and_dv_query(learner, X, n_instances=1):\n",
    "    uc_dv_scores = ranked_uc_and_dv_score(learner, X)\n",
    "    # Sort them in descending order\n",
    "    ranked_indices = np.argsort(uc_dv_scores)[::-1]\n",
    "    selected_indices = ranked_indices[:n_instances]\n",
    "    selected_instances = X[selected_indices]\n",
    "    \n",
    "    return selected_indices, selected_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2942592b83d431fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner, metrics = train_active_learner(query_strat=ranked_uc_and_dv_query, **training_config)\n",
    "save_model_and_metrics(experiment, dataset_name, f\"ranked_uc_and_dv_0_5\", learner, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2153dfb4df033cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_uc_dv, c_uc_dv_metrics = train_committee_learner(query_strat=ranked_uc_and_dv_query, **training_config_committee)\n",
    "save_model_and_metrics(experiment, dataset_name, \"ranked_uc_and_dv_0_5_committee\", c_uc_dv, c_uc_dv_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313d3b94cbcc787",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# This is not used in the final report but is here to enable testing stream-based methods in the final experiments\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Stream-Based sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23207125e2c1d2fd",
   "metadata": {},
   "source": [
    "with Uncertainty Sampling/ Classifier uncertainty as it's query score method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5e127aa6f12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for uncertainty_threshold, uncertainty_threshold_str in [(0.2, \"0_2\"), (0.5, \"0_5\"), (0.8, \"0_8\")]:\n",
    "#     learner, metrics = train_active_learner_stream(model_params=model_parameters, query_score_fn=classifier_uncertainty, query_score_threshold=uncertainty_threshold, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, X_stream=X_train, y_stream=y_train, X_initial=X_initial, y_initial=y_initial)\n",
    "#     save_model_and_metrics(experiment, dataset_name, f\"stream_classifier_uncertainty_th_{uncertainty_threshold_str}\", learner, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25eff2e9e5241c4",
   "metadata": {},
   "source": [
    "with Classification margin uncertainty as it's query score method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a83a3c69453ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner, metrics = train_active_learner_stream(model_params=model_parameters, query_score_fn=classifier_margin, query_score_threshold=0.5, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, X_stream=X_train, y_stream=y_train, X_initial=X_initial, y_initial=y_initial)\n",
    "# save_model_and_metrics(experiment, dataset_name, \"stream_classifier_margin\", learner, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8491fcd52e92a9",
   "metadata": {},
   "source": [
    "with Entropy margin uncertainty as it's query score method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1a37d0246eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner, metrics = train_active_learner_stream(model_params=model_parameters, query_score_fn=classifier_entropy, query_score_threshold=0.5, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, X_stream=X_train, y_stream=y_train, X_initial=X_initial, y_initial=y_initial)\n",
    "# save_model_and_metrics(experiment, dataset_name, \"stream_entropy\", learner, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1168161a537716",
   "metadata": {},
   "source": [
    "with the custom measurement of uncertainty and diversity of the already seen datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b1004d79d9f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for uncertainty_threshold, uncertainty_threshold_str in [(0.2, \"0_2\"), (0.5, \"0_5\"), (0.8, \"0_8\")]:\n",
    "#     for a, a_str in [(0.2, \"0_2\"), (0.5, \"0_5\"), (0.8, \"0_8\")]:\n",
    "#         alpha_uc_dv = a\n",
    "#         learner, metrics = train_active_learner_stream(\n",
    "#             model_params=model_parameters, \n",
    "#             query_score_fn=ranked_uc_and_dv_score, \n",
    "#             query_score_threshold=uncertainty_threshold, \n",
    "#             n_query_instances=n_query_instances, \n",
    "#             epochs=epochs, random_seed=RANDOM_SEED, \n",
    "#             X_stream=X_train, \n",
    "#             y_stream=y_train, \n",
    "#             X_initial=X_initial, \n",
    "#             y_initial=y_initial)\n",
    "#         save_model_and_metrics(experiment, dataset_name, \n",
    "#                                f\"stream_classifier_ranked_uc_and_dv_{a_str}_th_{uncertainty_threshold_str}\", \n",
    "#                                learner, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4229e26196ee7f82",
   "metadata": {},
   "source": [
    "## Disagreement Sampling (for classifiers) (uses a committee, so I should theoretically do every train run again for each methodwith a committee!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f60ad2552bc169d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e0181a457ba1a9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Methods that sadly don't work \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc96ce3b0ea18b",
   "metadata": {},
   "source": [
    "## Expected error reduction (doesn't work on my pc due to time complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc036fb6be26710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner, metrics = train_active_learner(model_params=model_parameters, query_strat=expected_error_reduction, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ccd53be1e214f",
   "metadata": {},
   "source": [
    "## Information Density (doesn't work on my pc due to time complexity)\n",
    "\n",
    "$$I(x)=\\frac{1}{\\left|X_u\\right|} \\sum_{x^{\\prime} \\in X} \\operatorname{sim}\\left(x, x^{\\prime}\\right)$$\n",
    "\n",
    "where $\\operatorname{sim}\\left(x, x^{\\prime}\\right)$ is a similarity function such as cosine similarity or Euclidean similarity, which is the reciprocal of Euclidean distance. The higher the information density, the more similar the given instance is to the rest of the data.\n",
    "\n",
    "\n",
    "According to the modAL docs: When using uncertainty sampling (or other similar strategies), we are unable to take the structure of the data into account which can lead to suboptimal queries.\n",
    "\n",
    "This could very well be used in combination with another strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a65b997a22566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inf_density(classifier, X_pool):\n",
    "#     return information_density(X_pool, metric='euclidean')\n",
    "# \n",
    "# learner, metrics = train_active_learner(model_params=model_parameters, query_strat=inf_density,  epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2db7db79104728",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This is for using it with a Committee (for multiple classes) so it's not optimal for comparing the query strategies themselves maybe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cbbd3fbf405c5c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Acquisition Functions might not be usable due to the fact that they require a BayesianOptimizer, not an ActiveLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f872cf1438ff1a5e",
   "metadata": {},
   "source": [
    "## Acquisition Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26dec36c690061",
   "metadata": {},
   "source": [
    "**Probability of improvement**: \n",
    "$$PI(x)=\\psi\\left(\\frac{\\mu(x) - f\\left(x^+\\right) - \\xi}{\\sigma(x)}\\right)$$\n",
    "where $\\mu(x)$ and $\\sigma(x)$ are mean and variance of regressor at $x$, $f$ is the model to be optimized with estimated maximum at $x^+$. $\\xi$ is a parameter controlling the degree of exploration and $\\psi(x)$ denotes cumulative distribution function of a standard Gaussian Distribution\n",
    "\n",
    "[Example from the ModAL Docs](https://modal-python.readthedocs.io/en/latest/_images/bo-PI.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77668772d478e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner, metrics = train_active_learner(model_params=model_parameters, query_strat=max_PI, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c93bd34cf6a53",
   "metadata": {},
   "source": [
    "**[Expected Improvement (from the ModAL Docs)](https://modal-python.readthedocs.io/en/latest/content/query_strategies/Acquisition-functions.html#expected-improvement)**: \n",
    "$$\n",
    "EI(x) = \n",
    "\\left( \\mu(x) - f(x^+) - \\xi \\right) \\cdot \\psi \\left( \\frac{\\mu(x) - f(x^+) - \\xi}{\\sigma(x)} \\right)\n",
    "+ \\sigma(x) \\phi \\left( \\frac{\\mu(x) - f(x^+) - \\xi}{\\sigma(x)} \\right),\n",
    "$$\n",
    "\n",
    "where $\\mu(x)$ and $\\sigma(x)$ are the mean and variance of the regressor at $x$, $f$ is the function to be optimized with estimated maximum at $x$, $\\xi$ is a parameter controlling the degree of exploration, and $\\psi(z), \\phi(z)$ denote the cumulative distribution function and density function of a standard Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ebdeda7bf2d25e",
   "metadata": {},
   "source": [
    "**Upper Confidence Bound**:\n",
    "$$ UCB(x) = \\mu(x) + \\beta \\sigma(x)$$\n",
    "where $\\mu(x)$ and $\\sigma(x)$ are mean and variance of the regressor and $\\beta$ is a parameter controlling the degree of exploration\n",
    "\n",
    "[Example from the ModAL Docs](https://modal-python.readthedocs.io/en/latest/_images/bo-UCB.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
