{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Common Variables and Imports",
   "id": "2f51c4a3d74244ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T09:48:14.061452Z",
     "start_time": "2025-01-07T09:48:14.056326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, time, warnings, pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, log_loss, pairwise_distances\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling, entropy_sampling, margin_sampling, classifier_uncertainty, classifier_entropy, classifier_margin\n",
    "from modAL.batch import uncertainty_batch_sampling\n",
    "from modAL.expected_error import expected_error_reduction\n",
    "from modAL.density import information_density\n",
    "\n",
    "from utils import load_MNIST, log_metrics, initialize_random_number_generators, create_model, save_model_and_metrics, load_model_and_metrics\n",
    "\n",
    "# Filter FutureWarnings to make outputs look more pleasant and ConvergenceWarnings which are given by sklearn LogisticRegressors when explicitly settings the multi_class to multinomial. Here, this could be omitted but I liked to leave it in for clarity to show that I'm not training 10 binary classifiers but one classifier with 10 outputs, each resembling the probabilities of a digit\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ],
   "id": "bf40dce6e55cb97a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T09:48:14.233599Z",
     "start_time": "2025-01-07T09:48:14.228657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RANDOM_SEED = 42\n",
    "epochs = 100 \n",
    "model_parameters={\n",
    "    \"max_iterations_per_epoch\": 10,\n",
    "    \"regularization\": 'l2', # l1', 'l2', 'elasticnet' or None\n",
    "    \"regularization_strength\": 1.0 # smaller values mean stronger regularization\n",
    "}\n",
    "\n",
    "initialize_random_number_generators(RANDOM_SEED)\n",
    "\n",
    "experiment = \"1\" # \"2\" or \"3\""
   ],
   "id": "a8442883d6651568",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T09:48:14.261978Z",
     "start_time": "2025-01-07T09:48:14.258016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "initialize_random_number_generators(seed=RANDOM_SEED)\n",
    "\n",
    "# n_queries = 100 # I set this to have a comparable amount of overall train epohcs on both models\n",
    "n_query_instances = 5\n",
    "n_initial = 100"
   ],
   "id": "de18d1472564317f",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load DataSet (MNIST)",
   "id": "319858033c6a6d13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T09:48:19.079465Z",
     "start_time": "2025-01-07T09:48:14.273985Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, y_train, X_test, y_test, X_whole, y_whole = load_MNIST()",
   "id": "8981b010982ba151",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check how often each digit appears in MNIST and whether the data for each class is balanced",
   "id": "f25b1ed32e6ccbf3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T09:48:19.089161Z",
     "start_time": "2025-01-07T09:48:19.081478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(10):\n",
    "    print(f\"Digit {i}: {np.count_nonzero(y_whole == i)} times\")"
   ],
   "id": "1d07dcd32f95410",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 0: 6903 times\n",
      "Digit 1: 7877 times\n",
      "Digit 2: 6990 times\n",
      "Digit 3: 7141 times\n",
      "Digit 4: 6824 times\n",
      "Digit 5: 6313 times\n",
      "Digit 6: 6876 times\n",
      "Digit 7: 7293 times\n",
      "Digit 8: 6825 times\n",
      "Digit 9: 6958 times\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Helper methods",
   "id": "59dc5c4062507364"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T09:48:19.109942Z",
     "start_time": "2025-01-07T09:48:19.091173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\" \n",
    "    Method for pool-based active learning query strategies. \n",
    "\"\"\"\n",
    "def train_active_learner(model_params, query_strat, n_query_instances:int, epochs:int, random_seed:int, pool_idx:list[int], X_initial, y_initial):\n",
    "    initialize_random_number_generators(seed=random_seed)\n",
    "    \n",
    "    X_pool = X_train[pool_idx]\n",
    "    y_pool = y_train[pool_idx]\n",
    "    \n",
    "    log_reg = create_model(model_params, random_seed=RANDOM_SEED)\n",
    "    \n",
    "    learner = ActiveLearner(estimator=log_reg, \n",
    "                        query_strategy=query_strat, \n",
    "                        X_training=X_initial, \n",
    "                        y_training=y_initial)\n",
    "    # Passing X_training and y_training to the ActiveLearner automatically calls the fit method for log_reg with these data points\n",
    "\n",
    "    metrics = {'queries': [], 'train_loss': [], 'train_loss_current': [], 'test_loss': [], 'test_acc': []}\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(epochs): # epochs=n_queries to have both models trained on the same number of overall epochs\n",
    "        query_idx, query_inst = learner.query(X_pool, n_instances=n_query_instances)\n",
    "        \n",
    "        # Simulate labeling\n",
    "        learner.teach(X_pool[query_idx], y_pool[query_idx])\n",
    "        \n",
    "        # Remove queried point(s)\n",
    "        X_pool, y_pool = np.delete(X_pool, query_idx, axis=0), np.delete(y_pool, query_idx, axis=0)\n",
    "    \n",
    "        metrics['queries'].append(query_idx)\n",
    "        # log_metrics logs train loss for the whole train dataset which doesn't reflect the actual value in the current step but gives the ability to compare both models on the training set. \n",
    "        # To log training state on the actual (current) training set, do this additionally\n",
    "        train_loss_current = log_loss(learner.y_training, learner.predict_proba(learner.X_training))\n",
    "        metrics['train_loss_current'].append(train_loss_current)\n",
    "        \n",
    "        log_metrics((epoch+1)*model_params[\"max_iterations_per_epoch\"], learner, X_train, y_train, X_test, y_test, metrics)\n",
    "        print(f\"       Current train loss: {train_loss_current:.4f}\")\n",
    "    print(f\"Training time: {time.time()-start:.2f} seconds\")\n",
    "    \n",
    "    return learner.estimator, metrics\n",
    "\n",
    "\"\"\" \n",
    "    Method for stream-based active learning query strategies. \n",
    "    \n",
    "    arguments:\n",
    "    - model_params: dict\n",
    "    - query_strat: dict\n",
    "    - query_score_threshold: float\n",
    "    - epochs: int\n",
    "    - random_seed: int\n",
    "    - X_stream: np.ndarray, typically full dataset\n",
    "    - y_stream: np.ndarray, typically full dataset\n",
    "    - X_initial: np.ndarray, initial training points\n",
    "    - y_initial: np.ndarray, initial training points\n",
    "\"\"\"\n",
    "def train_active_learner_stream(model_params, query_score_fn, n_query_instances:int, query_score_threshold:float, epochs:int, random_seed:int, X_stream, y_stream, X_initial, y_initial):    \n",
    "    initialize_random_number_generators(seed=random_seed)\n",
    "    \n",
    "    log_reg = create_model(model_params, random_seed=RANDOM_SEED)\n",
    "    \n",
    "    learner = ActiveLearner(estimator=log_reg, \n",
    "                            X_training=X_initial, \n",
    "                            y_training=y_initial)\n",
    "    # Passing X_training and y_training to the ActiveLearner automatically calls the fit method for log_reg with these data points\n",
    "\n",
    "    metrics = {'queries': [], 'train_loss': [], 'train_loss_current': [], 'test_loss': [], 'test_acc': []}\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # In pool based training, I get n_query_instances in each epoch. To have a comparable amount of data points for retraining the classifier, I use max_instances which equates to the amount of instances, a model in pool based approaches has seen. \n",
    "    max_instances = n_query_instances * epochs \n",
    "    used_instances = 0\n",
    "    \n",
    "    # Use a random permutation of the whole MNIST train data set to mimic a data stream. \n",
    "    stream_indices, stream_pointer = np.random.permutation(len(X_stream)), 0\n",
    "    \n",
    "    while used_instances < max_instances:\n",
    "        if stream_pointer >= len(stream_indices):  # Restart stream simulation if exhausted\n",
    "            stream_indices = np.random.permutation(len(X_stream))\n",
    "            stream_pointer = 0\n",
    "        \n",
    "        stream_idx = stream_indices[stream_pointer]\n",
    "        x_instance, y_instance = X_stream[stream_idx].reshape(1, -1), y_stream[stream_idx].reshape(-1,)\n",
    "        stream_pointer += 1\n",
    "        \n",
    "        query_score = query_score_fn(learner, x_instance)\n",
    "        \n",
    "        # Depending on the function, we want to select samples with either a high score (uncertainty) or a low score (margin)\n",
    "        \n",
    "        if query_score_fn == classifier_margin:\n",
    "            query_condition = query_score < query_score_threshold\n",
    "        else: # classifier_uncertainty, classifier_entropy\n",
    "            query_condition = query_score > query_score_threshold\n",
    "        \n",
    "        if query_condition:\n",
    "            learner.teach(x_instance, y_instance)\n",
    "    \n",
    "            metrics['queries'].append(stream_idx)\n",
    "            # log_metrics logs train loss for the whole train dataset which doesn't reflect the actual value in the current step but gives the ability to compare both models on the training set. \n",
    "            # To log training state on the actual (current) training set, do this additionally\n",
    "            train_loss_current = log_loss(learner.y_training, learner.predict_proba(learner.X_training))\n",
    "            metrics['train_loss_current'].append(train_loss_current)\n",
    "            \n",
    "            log_metrics((used_instances+1), learner, X_train, y_train, X_test, y_test, metrics)\n",
    "            print(f\"       Current train loss: {train_loss_current:.4f}\")\n",
    "            \n",
    "            used_instances += 1\n",
    "            \n",
    "    print(f\"Training time: {time.time()-start:.2f} seconds\")\n",
    "    \n",
    "    return learner.estimator, metrics"
   ],
   "id": "9bc50d7231c8089a",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating an initial labelled dataset from random datapoints and the unlabelled pool",
   "id": "e5e2e186d810f76e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T09:48:19.144350Z",
     "start_time": "2025-01-07T09:48:19.112954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False) # Indices with which the initial train set is created with\n",
    "X_initial = X_train[initial_idx]\n",
    "y_initial = y_train[initial_idx]\n",
    "pool_idx = np.setdiff1d(range(len(X_train)), initial_idx)"
   ],
   "id": "e6f0eebb42b902b6",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Train a Logistic Regressor on the whole train dataset\n",
    "Use multi_class='multinomial' solver: if it were 'ovr', each of the 10 output neurons would treat the corresponding number as a one-vs-rest szenario. So we would construct a Binary Distribution for each of the output neurons. But this doesn't take care of interdependencies between classes. \n",
    "In General: 'ovr' would train a separate classifier for each number and 'multinomial' does a softmax regression\n",
    "\n",
    "I could use ovr to have more control about the individual classes and to save computational costs \n",
    "\n",
    "Other option would be lbfgs which is also compatible with L2 and multiclass"
   ],
   "id": "ff515952001c52c3"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-07T09:48:19.145382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_full_model():\n",
    "    log_reg_full = create_model(model_parameters, random_seed=RANDOM_SEED)\n",
    "    \n",
    "    metrics = {'train_loss': [], 'test_loss': [], 'test_acc': []}\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        log_reg_full.fit(X_train, y_train)\n",
    "        log_metrics((epoch+1) * model_parameters[\"max_iterations_per_epoch\"], log_reg_full, X_train, y_train, X_test, y_test, metrics)\n",
    "\n",
    "    y_hat = log_reg_full.predict(X_test)\n",
    "    accuracy_whole_dataset = accuracy_score(y_test, y_hat)\n",
    "    print(f\"Test accuracy with whole Test dataset: {accuracy_whole_dataset:.4f}\")\n",
    "    print(f\"Training time: {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    return log_reg_full, metrics\n",
    "\n",
    "log_reg_whole_data, log_reg_whole_data_metrics = train_full_model()\n",
    "save_model_and_metrics(experiment, \"whole_dataset\", log_reg_whole_data, log_reg_whole_data_metrics)"
   ],
   "id": "30d1698ed021029a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: - Train Loss: 0.2365 - Test Loss: 0.2663 - Test Accuracy: 0.9259\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Active Learning\n",
    "\n",
    "---"
   ],
   "id": "ca339ac4a579fa91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train a Logistic Regressor on 100 data points without Active Learning\n",
    "\n",
    "This serves as a baseline to show how much can be learnt from n_initial data points. (This is expected to be low)"
   ],
   "id": "5029f71bc2e61288"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def train_initial_model():\n",
    "    initialize_random_number_generators(seed=RANDOM_SEED)\n",
    "    \n",
    "    log_reg_initial = create_model(model_parameters, random_seed=RANDOM_SEED)\n",
    "    \n",
    "    metrics = {'train_loss': [], 'train_loss_current': [], 'test_loss': [], 'test_acc': []}\n",
    "    \n",
    "    start = time.time()\n",
    "    log_reg_initial.fit(X_initial, y_initial)\n",
    "    print(f\"Train time: {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    y_pred = log_reg_initial.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "     \n",
    "    train_loss_current = log_loss(y_initial, log_reg_initial.predict_proba(X_initial))\n",
    "    metrics['train_loss_current'].append(train_loss_current)\n",
    "    \n",
    "    log_metrics(model_parameters[\"max_iterations_per_epoch\"], log_reg_initial, X_train, y_train, X_test, y_test, metrics)\n",
    "\n",
    "    print(f\"Test accuracy with whole Test dataset: {accuracy:.4f}\")\n",
    "    \n",
    "    return log_reg_initial, metrics\n",
    "\n",
    "log_reg_initial, log_reg_initial_metrics = train_initial_model()\n",
    "save_model_and_metrics(experiment, \"initial_active_model\", log_reg_initial, log_reg_initial_metrics)"
   ],
   "id": "f070945732a153f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train a Classifier with Various Query Strategies",
   "id": "b0711f048f244356"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From the documents and maybe worth trying: If you would like to start from scratch, you can use the .fit(X, y) method to make the learner forget everything it has seen and fit the model to the newly provided data.\n",
    "\n",
    "To train only on the newly acquired data, you should pass only_new=True to the .teach() method. "
   ],
   "id": "28e693b8576a43c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Random Sampling",
   "id": "e0d16dd6c5dba075"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def random_sampling(classifier, X_pool, n_instances):\n",
    "    n_samples = len(X_pool)\n",
    "    query_idx = np.random.choice(range(n_samples), size=n_instances, replace=False)\n",
    "    return query_idx, X_pool[query_idx]\n",
    "\n",
    "learner, metrics = train_active_learner(model_params=model_parameters, query_strat=random_sampling, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, \"random_sampling\", learner, metrics)"
   ],
   "id": "c0f2252e8a37778e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Uncertainty sampling strategies",
   "id": "ffb2eb9af020267d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Uncertainty Sampling**: Samples where classifier is least sure are selected",
   "id": "36cf1785e4959be6"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner(model_params=model_parameters, query_strat=uncertainty_sampling, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, \"uncertainty_sampling\", learner, metrics)"
   ],
   "id": "10fbdcdade2f050",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Entropy Sampling**: Samples where class probability has the largest Entropy",
   "id": "b2433d3fb21e7422"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner(model_params=model_parameters, query_strat=entropy_sampling, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, \"uncertainty_entropy_sampling\", learner, metrics)"
   ],
   "id": "bb318d73efbbd6a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Margin Sampling**: Selects instances where difference between first most likely and second most likely classes are the smallest",
   "id": "31e16b8e2de5c8cd"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner(model_params=model_parameters, query_strat=margin_sampling, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, \"uncertainty_margin_sampling\", learner, metrics)"
   ],
   "id": "842e3ccea720b828",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "afffb41bb387ae43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ranked Batch-Mode Sampling\n",
    "\n",
    "$$score=\\alpha\\left(1-\\Phi\\left(x, X_{\\text {labeled }}\\right)\\right)+(1-\\alpha) U(x)$$\n",
    "where $\\alpha=\\frac{\\left|X_{\\text {unlabeled }}\\right|}{\\left|X_{\\text {unlabeled }}\\right|+\\left|X_{\\text {labeled }}\\right|}, X_{\\text {labeled }}$ is the labeled dataset, $U(x)$ is the uncertainty of predictions for $x$, and $\\Phi$ is a so-called similarity function, for instance cosine similarity. This latter function measures how well the feature space is explored near $x$. (The lower the better.)\n",
    "\n",
    "According to the modAL docs: This strategy differs from uncertainty_sampling() because, although it is supported, traditional active learning query strategies suffer from sub-optimal record selection when passing n_instances > 1. This sampling strategy extends the interactive uncertainty query sampling by allowing for batch-mode uncertainty query sampling. Furthermore, it also enforces a ranking – that is, which records among the batch are most important for labeling?"
   ],
   "id": "fe351425d5454487"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner(model_params=model_parameters, query_strat=uncertainty_batch_sampling, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, \"ranked_batch_mode\", learner, metrics)"
   ],
   "id": "c29ca5f79c9fd0e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Custom combination between uncertainty of the data point(s) in X and diversity between X and parts of the train dataset",
   "id": "ce39ffdbd8b31fc5"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "alpha_uc_dv = 0.5\n",
    "def ranked_uc_and_dv_score(learner, X):\n",
    "    uncertainty = classifier_uncertainty(learner, X)\n",
    "    diversity = np.min(pairwise_distances(X, learner.X_training), axis=1)\n",
    "    combined_scores = alpha_uc_dv * uncertainty + (1 - alpha_uc_dv) * diversity\n",
    "    return combined_scores\n",
    "\n",
    "def ranked_uc_and_dv_query(learner, X, n_instances=1):\n",
    "    uc_dv_scores = ranked_uc_and_dv_score(learner, X)\n",
    "    # Sort them in descending order\n",
    "    ranked_indices = np.argsort(uc_dv_scores)[::-1]\n",
    "    selected_indices = ranked_indices[:n_instances]\n",
    "    selected_instances = X[selected_indices]\n",
    "    \n",
    "    return selected_indices, selected_instances\n",
    "\n",
    "for a, a_str in [(0.2, \"0_2\"), (0.5, \"0_5\"), (0.8, \"0_8\")]:\n",
    "    alpha_uc_dv = a\n",
    "    learner, metrics = train_active_learner(model_params=model_parameters, query_strat=ranked_uc_and_dv_query, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "    save_model_and_metrics(experiment, f\"ranked_uc_and_dv_{a_str}\", learner, metrics)"
   ],
   "id": "81e38f74bc551cca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Stream-Based sampling",
   "id": "9313d3b94cbcc787"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "with Uncertainty Sampling/ Classifier uncertainty as it's query score method",
   "id": "23207125e2c1d2fd"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for uncertainty_threshold, uncertainty_threshold_str in [(0.2, \"0_2\"), (0.5, \"0_5\"), (0.8, \"0_8\")]:\n",
    "    learner, metrics = train_active_learner_stream(model_params=model_parameters, query_score_fn=classifier_uncertainty, query_score_threshold=uncertainty_threshold, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, X_stream=X_train, y_stream=y_train, X_initial=X_initial, y_initial=y_initial)\n",
    "    save_model_and_metrics(experiment, f\"stream_classifier_uncertainty_th_{uncertainty_threshold_str}\", learner, metrics)"
   ],
   "id": "f6f5e127aa6f12f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "with Classification margin uncertainty as it's query score method",
   "id": "c25eff2e9e5241c4"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner_stream(model_params=model_parameters, query_score_fn=classifier_margin, query_score_threshold=0.5, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, X_stream=X_train, y_stream=y_train, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, \"stream_classifier_margin\", learner, metrics)"
   ],
   "id": "76a83a3c69453ec5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "with Entropy margin uncertainty as it's query score method",
   "id": "1b8491fcd52e92a9"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner_stream(model_params=model_parameters, query_score_fn=classifier_entropy, query_score_threshold=0.5, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, X_stream=X_train, y_stream=y_train, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, \"stream_entropy\", learner, metrics)"
   ],
   "id": "68c1a37d0246eb96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "with the custom measurement of uncertainty and diversity of the already seen datapoints",
   "id": "c1168161a537716"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for uncertainty_threshold, uncertainty_threshold_str in [(0.2, \"0_2\"), (0.5, \"0_5\"), (0.8, \"0_8\")]:\n",
    "    for a, a_str in [(0.2, \"0_2\"), (0.5, \"0_5\"), (0.8, \"0_8\")]:\n",
    "        alpha_uc_dv = a\n",
    "        learner, metrics = train_active_learner_stream(\n",
    "            model_params=model_parameters, \n",
    "            query_score_fn=ranked_uc_and_dv_score, \n",
    "            query_score_threshold=uncertainty_threshold, \n",
    "            n_query_instances=n_query_instances, \n",
    "            epochs=epochs, random_seed=RANDOM_SEED, \n",
    "            X_stream=X_train, \n",
    "            y_stream=y_train, \n",
    "            X_initial=X_initial, \n",
    "            y_initial=y_initial)\n",
    "        save_model_and_metrics(experiment, \n",
    "                               f\"stream_classifier_ranked_uc_and_dv_{a_str}_th_{uncertainty_threshold_str}\", \n",
    "                               learner, metrics)"
   ],
   "id": "d0b1004d79d9f7f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Disagreement Sampling (for classifiers) (uses a committee, so I should theoretically do every train run again for each methodwith a committee!)",
   "id": "4229e26196ee7f82"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5f60ad2552bc169d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Methods that sadly don't work \n",
    "\n",
    "---"
   ],
   "id": "6e0181a457ba1a9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Expected error reduction (doesn't work on my pc due to time complexity)",
   "id": "64cc96ce3b0ea18b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "# learner, metrics = train_active_learner(model_params=model_parameters, query_strat=expected_error_reduction, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)",
   "id": "cdc036fb6be26710",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Information Density (doesn't work on my pc due to time complexity)\n",
    "\n",
    "$$I(x)=\\frac{1}{\\left|X_u\\right|} \\sum_{x^{\\prime} \\in X} \\operatorname{sim}\\left(x, x^{\\prime}\\right)$$\n",
    "\n",
    "where $\\operatorname{sim}\\left(x, x^{\\prime}\\right)$ is a similarity function such as cosine similarity or Euclidean similarity, which is the reciprocal of Euclidean distance. The higher the information density, the more similar the given instance is to the rest of the data.\n",
    "\n",
    "\n",
    "According to the modAL docs: When using uncertainty sampling (or other similar strategies), we are unable to take the structure of the data into account which can lead to suboptimal queries.\n",
    "\n",
    "This could very well be used in combination with another strategy"
   ],
   "id": "724ccd53be1e214f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# def inf_density(classifier, X_pool):\n",
    "#     return information_density(X_pool, metric='euclidean')\n",
    "# \n",
    "# learner, metrics = train_active_learner(model_params=model_parameters, query_strat=inf_density,  epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)"
   ],
   "id": "c75a65b997a22566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "This is for using it with a Committee (for multiple classes) so it's not optimal for comparing the query strategies themselves maybe?"
   ],
   "id": "ea2db7db79104728"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "Acquisition Functions might not be usable due to the fact that they require a BayesianOptimizer, not an ActiveLearner"
   ],
   "id": "f2cbbd3fbf405c5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Acquisition Functions",
   "id": "f872cf1438ff1a5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Probability of improvement**: \n",
    "$$PI(x)=\\psi\\left(\\frac{\\mu(x) - f\\left(x^+\\right) - \\xi}{\\sigma(x)}\\right)$$\n",
    "where $\\mu(x)$ and $\\sigma(x)$ are mean and variance of regressor at $x$, $f$ is the model to be optimized with estimated maximum at $x^+$. $\\xi$ is a parameter controlling the degree of exploration and $\\psi(x)$ denotes cumulative distribution function of a standard Gaussian Distribution\n",
    "\n",
    "[Example from the ModAL Docs](https://modal-python.readthedocs.io/en/latest/_images/bo-PI.png)\n"
   ],
   "id": "ef26dec36c690061"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "# learner, metrics = train_active_learner(model_params=model_parameters, query_strat=max_PI, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)",
   "id": "b77668772d478e5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**[Expected Improvement (from the ModAL Docs)](https://modal-python.readthedocs.io/en/latest/content/query_strategies/Acquisition-functions.html#expected-improvement)**: \n",
    "$$\n",
    "EI(x) = \n",
    "\\left( \\mu(x) - f(x^+) - \\xi \\right) \\cdot \\psi \\left( \\frac{\\mu(x) - f(x^+) - \\xi}{\\sigma(x)} \\right)\n",
    "+ \\sigma(x) \\phi \\left( \\frac{\\mu(x) - f(x^+) - \\xi}{\\sigma(x)} \\right),\n",
    "$$\n",
    "\n",
    "where $\\mu(x)$ and $\\sigma(x)$ are the mean and variance of the regressor at $x$, $f$ is the function to be optimized with estimated maximum at $x$, $\\xi$ is a parameter controlling the degree of exploration, and $\\psi(z), \\phi(z)$ denote the cumulative distribution function and density function of a standard Gaussian distribution."
   ],
   "id": "571c93bd34cf6a53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Upper Confidence Bound**:\n",
    "$$ UCB(x) = \\mu(x) + \\beta \\sigma(x)$$\n",
    "where $\\mu(x)$ and $\\sigma(x)$ are mean and variance of the regressor and $\\beta$ is a parameter controlling the degree of exploration\n",
    "\n",
    "[Example from the ModAL Docs](https://modal-python.readthedocs.io/en/latest/_images/bo-UCB.png)"
   ],
   "id": "e8ebdeda7bf2d25e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
