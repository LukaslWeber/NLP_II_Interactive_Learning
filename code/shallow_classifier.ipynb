{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Common Variables and Imports",
   "id": "2f51c4a3d74244ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:34:06.404623Z",
     "start_time": "2025-01-13T09:33:54.777598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, time, warnings, pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, log_loss, pairwise_distances\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling, entropy_sampling, margin_sampling, classifier_uncertainty, classifier_entropy, classifier_margin\n",
    "from modAL.batch import uncertainty_batch_sampling\n",
    "from modAL.expected_error import expected_error_reduction\n",
    "from modAL.density import information_density\n",
    "\n",
    "from utils import load_MNIST, log_metrics, initialize_random_number_generators, create_log_reg_model, save_model_and_metrics, load_file, save_file\n",
    "\n",
    "# Filter FutureWarnings to make outputs look more pleasant and ConvergenceWarnings which are given by sklearn LogisticRegressors when explicitly settings the multi_class to multinomial. Here, this could be omitted but I liked to leave it in for clarity to show that I'm not training 10 binary classifiers but one classifier with 10 outputs, each resembling the probabilities of a digit\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ],
   "id": "bf40dce6e55cb97a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:34:06.440644Z",
     "start_time": "2025-01-13T09:34:06.404623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RANDOM_SEED = 42\n",
    "epochs = 250 \n",
    "model_parameters=load_file(\"shallow_classifier_parameters.pkl\")\n",
    "\n",
    "initialize_random_number_generators(RANDOM_SEED)\n",
    "\n",
    "experiment = \"3\" # \"1\" or \"2\" or \"3\"\n",
    "dataset_name = \"MNIST\""
   ],
   "id": "a8442883d6651568",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_iterations_per_epoch': 50, 'regularization': 'l2', 'regularization_strength': 0.1, 'solver': 'lbfgs', 'early_stopping_tol': 0.001}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Load Dataset (MNIST)\n",
    "\n",
    "This loads the vectorized version of MNIST and normalizes values from $[0,255]$ to the range $[0,1]$\n",
    "\n",
    "The datasets are saved in the experiment folders for convenience and checking whether the splits, etc. are actually the same."
   ],
   "id": "319858033c6a6d13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T21:09:02.529326Z",
     "start_time": "2025-01-10T21:08:40.727450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, y_train, X_test, y_test, _, _, X_whole, y_whole = load_MNIST(random_seed=RANDOM_SEED)\n",
    "save_file(os.path.join(\"../results\", dataset_name,  f\"exp{experiment}\", \"datasets.pkl\"), {\"X_train\": X_train, \"y_train\": y_train, \"X_test\": X_test, \"y_test\": y_test})"
   ],
   "id": "8981b010982ba151",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 7,
   "source": [
    "experiment_parameters = {\"1\": {\"n_initial\" : 10},\n",
    "                         \"2\": {\"n_initial\" : int(len(X_train)*0.15)},\n",
    "                         \"3\": {\"n_initial\" : int(len(X_train)*0.8)}}\n",
    "\n",
    "n_query_instances = 5\n",
    "n_initial = experiment_parameters[experiment][\"n_initial\"]"
   ],
   "id": "de18d1472564317f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check how balanced the dataset is",
   "id": "9d4baeb0f1e481c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check how often each digit appears in MNIST and whether the data for each class is balanced",
   "id": "f25b1ed32e6ccbf3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T21:09:02.549388Z",
     "start_time": "2025-01-10T21:09:02.535712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(10):\n",
    "    print(f\"Digit {i}: {np.count_nonzero(y_whole == i)} times\")"
   ],
   "id": "1d07dcd32f95410",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 0: 6903 times\n",
      "Digit 1: 7877 times\n",
      "Digit 2: 6990 times\n",
      "Digit 3: 7141 times\n",
      "Digit 4: 6824 times\n",
      "Digit 5: 6313 times\n",
      "Digit 6: 6876 times\n",
      "Digit 7: 7293 times\n",
      "Digit 8: 6825 times\n",
      "Digit 9: 6958 times\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Helper methods",
   "id": "59dc5c4062507364"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T21:09:02.602774Z",
     "start_time": "2025-01-10T21:09:02.553409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\" \n",
    "    Method for pool-based active learning query strategies. \n",
    "\"\"\"\n",
    "def train_active_learner(model_params, query_strat, n_query_instances:int, epochs:int, random_seed:int, pool_idx:list[int], X_initial, y_initial):\n",
    "    initialize_random_number_generators(seed=random_seed)\n",
    "    \n",
    "    X_pool = X_train[pool_idx]\n",
    "    y_pool = y_train[pool_idx]\n",
    "    \n",
    "    log_reg = create_log_reg_model(model_params, random_seed=RANDOM_SEED)\n",
    "    log_reg.classes_ = np.arange(10)\n",
    "    \n",
    "    learner = ActiveLearner(estimator=log_reg, \n",
    "                            query_strategy=query_strat, \n",
    "                            X_training=X_initial, \n",
    "                            y_training=y_initial)\n",
    "    \n",
    "    # Passing X_training and y_training to the ActiveLearner automatically calls the fit method for log_reg with these data points\n",
    "\n",
    "    metrics = {'queries': [], 'train_loss': [], 'train_loss_current': [], 'test_loss': [], 'test_acc': []}\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(epochs): # epochs=n_queries to have both models trained on the same number of overall epochs\n",
    "        query_idx, query_inst = learner.query(X_pool, n_instances=n_query_instances)\n",
    "        \n",
    "        # Simulate labeling\n",
    "        learner.teach(X_pool[query_idx], y_pool[query_idx])\n",
    "        \n",
    "        # Remove queried point(s)\n",
    "        X_pool, y_pool = np.delete(X_pool, query_idx, axis=0), np.delete(y_pool, query_idx, axis=0)\n",
    "    \n",
    "        metrics['queries'].append(query_idx)\n",
    "        # log_metrics logs train loss for the whole train dataset which doesn't reflect the actual value in the current step but gives the ability to compare both models on the training set. \n",
    "        # To log training state on the actual (current) training set, do this additionally\n",
    "        train_loss_current = log_loss(learner.y_training, learner.predict_proba(learner.X_training))\n",
    "        metrics['train_loss_current'].append(train_loss_current)\n",
    "        \n",
    "        log_metrics((epoch+1)*model_params[\"max_iterations_per_epoch\"], learner, X_train, y_train, X_test, y_test, metrics)\n",
    "        print(f\"       Current train loss: {train_loss_current:.4f}\")\n",
    "    print(f\"Training time: {time.time()-start:.2f} seconds\")\n",
    "    \n",
    "    return learner.estimator, metrics\n",
    "\n",
    "\"\"\" \n",
    "    Method for stream-based active learning query strategies. \n",
    "    \n",
    "    arguments:\n",
    "    - model_params: dict\n",
    "    - query_strat: dict\n",
    "    - query_score_threshold: float\n",
    "    - epochs: int\n",
    "    - random_seed: int\n",
    "    - X_stream: np.ndarray, typically full dataset\n",
    "    - y_stream: np.ndarray, typically full dataset\n",
    "    - X_initial: np.ndarray, initial training points\n",
    "    - y_initial: np.ndarray, initial training points\n",
    "\"\"\"\n",
    "def train_active_learner_stream(model_params, query_score_fn, n_query_instances:int, query_score_threshold:float, epochs:int, random_seed:int, X_stream, y_stream, X_initial, y_initial):    \n",
    "    initialize_random_number_generators(seed=random_seed)\n",
    "    \n",
    "    log_reg = create_log_reg_model(model_params, random_seed=RANDOM_SEED)\n",
    "    \n",
    "    learner = ActiveLearner(estimator=log_reg, \n",
    "                            X_training=X_initial, \n",
    "                            y_training=y_initial)\n",
    "    # Passing X_training and y_training to the ActiveLearner automatically calls the fit method for log_reg with these data points\n",
    "\n",
    "    metrics = {'queries': [], 'train_loss': [], 'train_loss_current': [], 'test_loss': [], 'test_acc': []}\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # In pool based training, I get n_query_instances in each epoch. To have a comparable amount of data points for retraining the classifier, I use max_instances which equates to the amount of instances, a model in pool based approaches has seen. \n",
    "    max_instances = n_query_instances * epochs \n",
    "    used_instances = 0\n",
    "    \n",
    "    # Prevent infinite looping in case that no sample fulfills the query condition\n",
    "    retry_count, max_retries = 0, 10000\n",
    "    \n",
    "    # Use a random permutation of the whole MNIST train data set to mimic a data stream. \n",
    "    stream_indices, stream_pointer = np.random.permutation(len(X_stream)), 0\n",
    "    \n",
    "    while used_instances < max_instances:\n",
    "        if stream_pointer >= len(stream_indices):  # Restart stream simulation if the loop went through the whole list of data points\n",
    "            stream_indices = np.random.permutation(len(X_stream))\n",
    "            stream_pointer = 0\n",
    "        \n",
    "        stream_idx = stream_indices[stream_pointer]\n",
    "        x_instance, y_instance = X_stream[stream_idx].reshape(1, -1), y_stream[stream_idx].reshape(-1,)\n",
    "        stream_pointer += 1\n",
    "        retry_count += 1\n",
    "        \n",
    "        query_score = query_score_fn(learner, x_instance)\n",
    "        \n",
    "        # Depending on the function, we want to select samples with either a high score (uncertainty) or a low score (margin)\n",
    "        \n",
    "        if query_score_fn == classifier_margin:\n",
    "            query_condition = query_score < query_score_threshold\n",
    "        else: # classifier_uncertainty, classifier_entropy\n",
    "            query_condition = query_score > query_score_threshold\n",
    "        \n",
    "        if query_condition:\n",
    "            learner.teach(x_instance, y_instance)\n",
    "    \n",
    "            metrics['queries'].append(stream_idx)\n",
    "            # log_metrics logs train loss for the whole train dataset which doesn't reflect the actual value in the current step but gives the ability to compare both models on the training set. \n",
    "            # To log training state on the actual (current) training set, do this additionally\n",
    "            train_loss_current = log_loss(learner.y_training, learner.predict_proba(learner.X_training))\n",
    "            metrics['train_loss_current'].append(train_loss_current)\n",
    "            \n",
    "            log_metrics((used_instances+1), learner, X_train, y_train, X_test, y_test, metrics)\n",
    "            print(f\"       Current train loss: {train_loss_current:.4f}\")\n",
    "            \n",
    "            used_instances += 1\n",
    "            retry_count = 0\n",
    "        \n",
    "        if retry_count > max_retries:\n",
    "            print(f\"No suitable example could be found after {max_retries} retries, so training is stopped early.\")\n",
    "            break\n",
    "            \n",
    "    print(f\"Training time: {time.time()-start:.2f} seconds\")\n",
    "    \n",
    "    return learner.estimator, metrics"
   ],
   "id": "9bc50d7231c8089a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating an initial labelled dataset from random datapoints and the unlabelled pool\n",
    "\n",
    "If n_initial is smaller than 20, the model cannot be initialized properly and will throw errors so exactly one sample from each class is picked from a random permutation as the initial training set. "
   ],
   "id": "e5e2e186d810f76e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T21:09:02.715304Z",
     "start_time": "2025-01-10T21:09:02.611019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if n_initial == 10:\n",
    "    initial_idx = []\n",
    "    for cls in np.arange(10):\n",
    "        cls_idxs = np.where(y_train == cls)[0]\n",
    "        initial_idx.append(np.random.choice(cls_idxs))\n",
    "    # construct the X and y initial with one item from each class from a random permutation. the initial idx should keep the original mnist index.\n",
    "    # This is done to ensure that the model has an initial train set where it has seen each class\n",
    "    # Sadly, otherwise it will throw errors\n",
    "else:\n",
    "    initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False) # Indices with which the initial train set is created with\n",
    "\n",
    "X_initial = X_train[initial_idx]\n",
    "y_initial = y_train[initial_idx]\n",
    "pool_idx = np.setdiff1d(range(len(X_train)), initial_idx)"
   ],
   "id": "e6f0eebb42b902b6",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Train a Logistic Regressor on the whole train dataset\n",
    "Use multi_class='multinomial' solver: if it were 'ovr', each of the 10 output neurons would treat the corresponding number as a one-vs-rest szenario. So we would construct a Binary Distribution for each of the output neurons. But this doesn't take care of interdependencies between classes. \n",
    "In General: 'ovr' would train a separate classifier for each number and 'multinomial' does a softmax regression\n",
    "\n",
    "The train_full_model trains the model in a number of epochs. It was done this way to check if successively training a model for max_iter iterations for a number of epochs would provide the same results as training it once for max_iter*epoch iterations."
   ],
   "id": "ff515952001c52c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T21:20:49.404235Z",
     "start_time": "2025-01-10T21:09:02.720614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_full_model():\n",
    "    log_reg_full = create_log_reg_model(model_parameters, random_seed=RANDOM_SEED)\n",
    "    \n",
    "    metrics = {'train_loss': [], 'test_loss': [], 'test_acc': []}\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        log_reg_full.fit(X_train, y_train)\n",
    "        log_metrics((epoch+1) * model_parameters[\"max_iterations_per_epoch\"], log_reg_full, X_train, y_train, X_test, y_test, metrics)\n",
    "\n",
    "    y_hat = log_reg_full.predict(X_test)\n",
    "    accuracy_whole_dataset = accuracy_score(y_test, y_hat)\n",
    "    print(f\"Test accuracy with whole Test dataset: {accuracy_whole_dataset:.4f}\")\n",
    "    print(f\"Training time: {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    return log_reg_full, metrics\n",
    "\n",
    "log_reg_whole_data, log_reg_whole_data_metrics = train_full_model()\n",
    "save_model_and_metrics(experiment, dataset_name, \"whole_dataset\", log_reg_whole_data, log_reg_whole_data_metrics)"
   ],
   "id": "30d1698ed021029a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: - Train Loss: 0.2365 - Test Loss: 0.2663 - Test Accuracy: 0.9259\n",
      "Iteration 20: - Train Loss: 0.2300 - Test Loss: 0.2675 - Test Accuracy: 0.9269\n",
      "Iteration 30: - Train Loss: 0.2272 - Test Loss: 0.2693 - Test Accuracy: 0.9265\n",
      "Iteration 40: - Train Loss: 0.2255 - Test Loss: 0.2706 - Test Accuracy: 0.9266\n",
      "Iteration 50: - Train Loss: 0.2244 - Test Loss: 0.2715 - Test Accuracy: 0.9263\n",
      "Iteration 60: - Train Loss: 0.2236 - Test Loss: 0.2721 - Test Accuracy: 0.9263\n",
      "Iteration 70: - Train Loss: 0.2230 - Test Loss: 0.2726 - Test Accuracy: 0.9263\n",
      "Iteration 80: - Train Loss: 0.2226 - Test Loss: 0.2729 - Test Accuracy: 0.9262\n",
      "Iteration 90: - Train Loss: 0.2222 - Test Loss: 0.2731 - Test Accuracy: 0.9262\n",
      "Iteration 100: - Train Loss: 0.2220 - Test Loss: 0.2733 - Test Accuracy: 0.9262\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 18\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining time: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtime\u001B[38;5;241m.\u001B[39mtime()\u001B[38;5;250m \u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;250m \u001B[39mstart\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m seconds\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m log_reg_full, metrics\n\u001B[1;32m---> 18\u001B[0m log_reg_whole_data, log_reg_whole_data_metrics \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_full_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m save_model_and_metrics(experiment, dataset_name, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhole_dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, log_reg_whole_data, log_reg_whole_data_metrics)\n",
      "Cell \u001B[1;32mIn[12], line 8\u001B[0m, in \u001B[0;36mtrain_full_model\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[1;32m----> 8\u001B[0m     \u001B[43mlog_reg_full\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     log_metrics((epoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m*\u001B[39m model_parameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_iterations_per_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m], log_reg_full, X_train, y_train, X_test, y_test, metrics)\n\u001B[0;32m     11\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m log_reg_full\u001B[38;5;241m.\u001B[39mpredict(X_test)\n",
      "File \u001B[1;32m~\\nlp2\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1387\u001B[0m     )\n\u001B[0;32m   1388\u001B[0m ):\n\u001B[1;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\nlp2\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1350\u001B[0m, in \u001B[0;36mLogisticRegression.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m   1347\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1348\u001B[0m     n_threads \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1350\u001B[0m fold_coefs_ \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprefer\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1352\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1353\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1354\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpos_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1355\u001B[0m \u001B[43m        \u001B[49m\u001B[43mCs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mC_\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1356\u001B[0m \u001B[43m        \u001B[49m\u001B[43ml1_ratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43ml1_ratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1357\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfit_intercept\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_intercept\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1358\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1359\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1360\u001B[0m \u001B[43m        \u001B[49m\u001B[43msolver\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msolver\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1361\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmulti_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmulti_class\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1362\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1363\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1364\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1365\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1366\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcoef\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwarm_start_coef_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1367\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpenalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpenalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1368\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_squared_sum\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_squared_sum\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1369\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1370\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_threads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1371\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1372\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mclass_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwarm_start_coef_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mclasses_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwarm_start_coef\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1373\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1375\u001B[0m fold_coefs_, _, n_iter_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mfold_coefs_)\n\u001B[0;32m   1376\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_iter_ \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(n_iter_, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint32)[:, \u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\nlp2\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     72\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     73\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     74\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     76\u001B[0m )\n\u001B[1;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\nlp2\\Lib\\site-packages\\joblib\\parallel.py:1918\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1916\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1917\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[0;32m   1920\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1921\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1922\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1923\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1924\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1925\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32m~\\nlp2\\Lib\\site-packages\\joblib\\parallel.py:1847\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1847\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1848\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1849\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[1;32m~\\nlp2\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    137\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 139\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\nlp2\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:543\u001B[0m, in \u001B[0;36m_logistic_regression_path\u001B[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001B[0m\n\u001B[0;32m    540\u001B[0m         alpha \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m/\u001B[39m C) \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m l1_ratio)\n\u001B[0;32m    541\u001B[0m         beta \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m/\u001B[39m C) \u001B[38;5;241m*\u001B[39m l1_ratio\n\u001B[1;32m--> 543\u001B[0m     w0, n_iter_i, warm_start_sag \u001B[38;5;241m=\u001B[39m \u001B[43msag_solver\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    544\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    545\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    546\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    547\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    548\u001B[0m \u001B[43m        \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    549\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    550\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    551\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    552\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    553\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    554\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    555\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_squared_sum\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    556\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwarm_start_sag\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    557\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_saga\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msolver\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msaga\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    558\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    560\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    561\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    562\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msolver must be one of \u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mliblinear\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlbfgs\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    563\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnewton-cg\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msag\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m}, got \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m instead\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m solver\n\u001B[0;32m    564\u001B[0m     )\n",
      "File \u001B[1;32m~\\nlp2\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:323\u001B[0m, in \u001B[0;36msag_solver\u001B[1;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001B[0m\n\u001B[0;32m    317\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mZeroDivisionError\u001B[39;00m(\n\u001B[0;32m    318\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCurrent sag implementation does not handle \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    319\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe case step_size * alpha_scaled == 1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    320\u001B[0m     )\n\u001B[0;32m    322\u001B[0m sag \u001B[38;5;241m=\u001B[39m sag64 \u001B[38;5;28;01mif\u001B[39;00m X\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mfloat64 \u001B[38;5;28;01melse\u001B[39;00m sag32\n\u001B[1;32m--> 323\u001B[0m num_seen, n_iter_ \u001B[38;5;241m=\u001B[39m \u001B[43msag\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    324\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    325\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcoef_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    326\u001B[0m \u001B[43m    \u001B[49m\u001B[43mintercept_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    327\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_samples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    328\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    329\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_classes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    330\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    331\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    332\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    333\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstep_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    334\u001B[0m \u001B[43m    \u001B[49m\u001B[43malpha_scaled\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    335\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta_scaled\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    336\u001B[0m \u001B[43m    \u001B[49m\u001B[43msum_gradient_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    337\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgradient_memory_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    338\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseen_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    339\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_seen_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    340\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfit_intercept\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    341\u001B[0m \u001B[43m    \u001B[49m\u001B[43mintercept_sum_gradient\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    342\u001B[0m \u001B[43m    \u001B[49m\u001B[43mintercept_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    343\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_saga\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    345\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_iter_ \u001B[38;5;241m==\u001B[39m max_iter:\n\u001B[0;32m    348\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    349\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe max_iter was reached which means the coef_ did not converge\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    350\u001B[0m         ConvergenceWarning,\n\u001B[0;32m    351\u001B[0m     )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Active Learning\n",
    "\n",
    "---"
   ],
   "id": "ca339ac4a579fa91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train a Logistic Regressor on 100 data points without Active Learning\n",
    "\n",
    "This serves as a baseline to show how much can be learnt from n_initial data points. (This is expected to be low)"
   ],
   "id": "5029f71bc2e61288"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T21:20:58.359069Z",
     "start_time": "2025-01-10T21:20:56.908935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_initial_model():\n",
    "    initialize_random_number_generators(seed=RANDOM_SEED)\n",
    "    \n",
    "    log_reg_initial = create_log_reg_model(model_parameters, random_seed=RANDOM_SEED)\n",
    "    \n",
    "    metrics = {'train_loss': [], 'train_loss_current': [], 'test_loss': [], 'test_acc': []}\n",
    "    \n",
    "    start = time.time()\n",
    "    if len(X_initial) > 0:\n",
    "        log_reg_initial.fit(X_initial, y_initial)\n",
    "        print(f\"Train time: {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    y_pred = log_reg_initial.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "     \n",
    "    train_loss_current = log_loss(y_initial, log_reg_initial.predict_proba(X_initial))\n",
    "    metrics['train_loss_current'].append(train_loss_current)\n",
    "    \n",
    "    log_metrics(model_parameters[\"max_iterations_per_epoch\"], log_reg_initial, X_train, y_train, X_test, y_test, metrics)\n",
    "\n",
    "    print(f\"Test accuracy with whole Test dataset: {accuracy:.4f}\")\n",
    "    \n",
    "    return log_reg_initial, metrics\n",
    "\n",
    "log_reg_initial, log_reg_initial_metrics = train_initial_model()\n",
    "save_model_and_metrics(experiment, dataset_name, \"initial_active_model\", log_reg_initial, log_reg_initial_metrics)"
   ],
   "id": "f070945732a153f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 0.96 seconds\n",
      "Iteration 10: - Train Loss: 0.4478 - Test Loss: 0.4181 - Test Accuracy: 0.8773\n",
      "Test accuracy with whole Test dataset: 0.8773\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train a Classifier with Various Query Strategies",
   "id": "b0711f048f244356"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From the documents and maybe worth trying: If you would like to start from scratch, you can use the .fit(X, y) method to make the learner forget everything it has seen and fit the model to the newly provided data.\n",
    "\n",
    "To train only on the newly acquired data, you should pass only_new=True to the .teach() method. "
   ],
   "id": "28e693b8576a43c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Random Sampling",
   "id": "e0d16dd6c5dba075"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T21:21:06.373463Z",
     "start_time": "2025-01-10T21:21:02.002143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def random_sampling(classifier, X_pool, n_instances):\n",
    "    n_samples = len(X_pool)\n",
    "    query_idx = np.random.choice(range(n_samples), size=n_instances, replace=False)\n",
    "    return query_idx, X_pool[query_idx]\n",
    "\n",
    "learner, metrics = train_active_learner(model_params=model_parameters, query_strat=random_sampling, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, dataset_name, \"random_sampling\", learner, metrics)"
   ],
   "id": "c0f2252e8a37778e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: - Train Loss: 0.4512 - Test Loss: 0.4197 - Test Accuracy: 0.8788\n",
      "       Current train loss: 0.0887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Uncertainty sampling strategies",
   "id": "ffb2eb9af020267d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Uncertainty Sampling**: Samples where classifier is least sure are selected",
   "id": "36cf1785e4959be6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T21:20:49.406254Z",
     "start_time": "2025-01-10T21:20:49.406254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner(model_params=model_parameters, query_strat=uncertainty_sampling, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, dataset_name, \"uncertainty_sampling\", learner, metrics)"
   ],
   "id": "10fbdcdade2f050",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Entropy Sampling**: Samples where class probability has the largest Entropy",
   "id": "b2433d3fb21e7422"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T21:20:49.406254Z",
     "start_time": "2025-01-10T21:20:49.406254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner(model_params=model_parameters, query_strat=entropy_sampling, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, dataset_name, \"uncertainty_entropy_sampling\", learner, metrics)"
   ],
   "id": "bb318d73efbbd6a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Margin Sampling**: Selects instances where difference between first most likely and second most likely classes are the smallest",
   "id": "31e16b8e2de5c8cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T21:20:49.421898Z",
     "start_time": "2025-01-10T21:20:49.406254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner(model_params=model_parameters, query_strat=margin_sampling, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, dataset_name, \"uncertainty_margin_sampling\", learner, metrics)"
   ],
   "id": "842e3ccea720b828",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "afffb41bb387ae43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ranked Batch-Mode Sampling\n",
    "\n",
    "$$score=\\alpha\\left(1-\\Phi\\left(x, X_{\\text {labeled }}\\right)\\right)+(1-\\alpha) U(x)$$\n",
    "where $\\alpha=\\frac{\\left|X_{\\text {unlabeled }}\\right|}{\\left|X_{\\text {unlabeled }}\\right|+\\left|X_{\\text {labeled }}\\right|}, X_{\\text {labeled }}$ is the labeled dataset, $U(x)$ is the uncertainty of predictions for $x$, and $\\Phi$ is a so-called similarity function, for instance cosine similarity. This latter function measures how well the feature space is explored near $x$. (The lower the better.)\n",
    "\n",
    "According to the modAL docs: This strategy differs from uncertainty_sampling() because, although it is supported, traditional active learning query strategies suffer from sub-optimal record selection when passing n_instances > 1. This sampling strategy extends the interactive uncertainty query sampling by allowing for batch-mode uncertainty query sampling. Furthermore, it also enforces a ranking – that is, which records among the batch are most important for labeling?"
   ],
   "id": "fe351425d5454487"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T21:20:49.426321Z",
     "start_time": "2025-01-10T21:20:49.421898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner(model_params=model_parameters, query_strat=uncertainty_batch_sampling, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, dataset_name, \"ranked_batch_mode\", learner, metrics)"
   ],
   "id": "c29ca5f79c9fd0e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Custom combination between uncertainty of the data point(s) in X and diversity between X and parts of the train dataset",
   "id": "ce39ffdbd8b31fc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "alpha_uc_dv = 0.5\n",
    "def ranked_uc_and_dv_score(learner, X):\n",
    "    uncertainty = classifier_uncertainty(learner, X)\n",
    "    diversity = np.min(pairwise_distances(X, learner.X_training), axis=1)\n",
    "    combined_scores = alpha_uc_dv * uncertainty + (1 - alpha_uc_dv) * diversity\n",
    "    return combined_scores\n",
    "\n",
    "def ranked_uc_and_dv_query(learner, X, n_instances=1):\n",
    "    uc_dv_scores = ranked_uc_and_dv_score(learner, X)\n",
    "    # Sort them in descending order\n",
    "    ranked_indices = np.argsort(uc_dv_scores)[::-1]\n",
    "    selected_indices = ranked_indices[:n_instances]\n",
    "    selected_instances = X[selected_indices]\n",
    "    \n",
    "    return selected_indices, selected_instances\n",
    "\n",
    "for a, a_str in [(0.2, \"0_2\"), (0.5, \"0_5\"), (0.8, \"0_8\")]:\n",
    "    alpha_uc_dv = a\n",
    "    learner, metrics = train_active_learner(model_params=model_parameters, query_strat=ranked_uc_and_dv_query, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)\n",
    "    save_model_and_metrics(experiment, dataset_name, f\"ranked_uc_and_dv_{a_str}\", learner, metrics)"
   ],
   "id": "81e38f74bc551cca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Stream-Based sampling",
   "id": "9313d3b94cbcc787"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "with Uncertainty Sampling/ Classifier uncertainty as it's query score method",
   "id": "23207125e2c1d2fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for uncertainty_threshold, uncertainty_threshold_str in [(0.2, \"0_2\"), (0.5, \"0_5\"), (0.8, \"0_8\")]:\n",
    "    learner, metrics = train_active_learner_stream(model_params=model_parameters, query_score_fn=classifier_uncertainty, query_score_threshold=uncertainty_threshold, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, X_stream=X_train, y_stream=y_train, X_initial=X_initial, y_initial=y_initial)\n",
    "    save_model_and_metrics(experiment, dataset_name, f\"stream_classifier_uncertainty_th_{uncertainty_threshold_str}\", learner, metrics)"
   ],
   "id": "f6f5e127aa6f12f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "with Classification margin uncertainty as it's query score method",
   "id": "c25eff2e9e5241c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner_stream(model_params=model_parameters, query_score_fn=classifier_margin, query_score_threshold=0.5, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, X_stream=X_train, y_stream=y_train, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, dataset_name, \"stream_classifier_margin\", learner, metrics)"
   ],
   "id": "76a83a3c69453ec5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "with Entropy margin uncertainty as it's query score method",
   "id": "1b8491fcd52e92a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "learner, metrics = train_active_learner_stream(model_params=model_parameters, query_score_fn=classifier_entropy, query_score_threshold=0.5, n_query_instances=n_query_instances, epochs=epochs, random_seed=RANDOM_SEED, X_stream=X_train, y_stream=y_train, X_initial=X_initial, y_initial=y_initial)\n",
    "save_model_and_metrics(experiment, dataset_name, \"stream_entropy\", learner, metrics)"
   ],
   "id": "68c1a37d0246eb96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "with the custom measurement of uncertainty and diversity of the already seen datapoints",
   "id": "c1168161a537716"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for uncertainty_threshold, uncertainty_threshold_str in [(0.2, \"0_2\"), (0.5, \"0_5\"), (0.8, \"0_8\")]:\n",
    "    for a, a_str in [(0.1, \"0_2\"), (0.2, \"0_2\"), (0.5, \"0_5\"), (0.8, \"0_8\")]:\n",
    "        alpha_uc_dv = a # this is used by the train active learner stream method\n",
    "        learner, metrics = train_active_learner_stream(\n",
    "            model_params=model_parameters, \n",
    "            query_score_fn=ranked_uc_and_dv_score, \n",
    "            query_score_threshold=uncertainty_threshold, \n",
    "            n_query_instances=n_query_instances, \n",
    "            epochs=epochs, random_seed=RANDOM_SEED, \n",
    "            X_stream=X_train, \n",
    "            y_stream=y_train, \n",
    "            X_initial=X_initial, \n",
    "            y_initial=y_initial)\n",
    "        save_model_and_metrics(experiment, dataset_name, \n",
    "                               f\"stream_classifier_ranked_uc_and_dv_{a_str}_th_{uncertainty_threshold_str}\", \n",
    "                               learner, metrics)"
   ],
   "id": "d0b1004d79d9f7f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Disagreement Sampling (for classifiers) (uses a committee, so I should theoretically do every train run again for each methodwith a committee!)",
   "id": "4229e26196ee7f82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5f60ad2552bc169d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Methods that sadly don't work \n",
    "\n",
    "---"
   ],
   "id": "6e0181a457ba1a9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Expected error reduction (doesn't work on my pc due to time complexity)",
   "id": "64cc96ce3b0ea18b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# learner, metrics = train_active_learner(model_params=model_parameters, query_strat=expected_error_reduction, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)",
   "id": "cdc036fb6be26710",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Information Density (doesn't work on my pc due to time complexity)\n",
    "\n",
    "$$I(x)=\\frac{1}{\\left|X_u\\right|} \\sum_{x^{\\prime} \\in X} \\operatorname{sim}\\left(x, x^{\\prime}\\right)$$\n",
    "\n",
    "where $\\operatorname{sim}\\left(x, x^{\\prime}\\right)$ is a similarity function such as cosine similarity or Euclidean similarity, which is the reciprocal of Euclidean distance. The higher the information density, the more similar the given instance is to the rest of the data.\n",
    "\n",
    "\n",
    "According to the modAL docs: When using uncertainty sampling (or other similar strategies), we are unable to take the structure of the data into account which can lead to suboptimal queries.\n",
    "\n",
    "This could very well be used in combination with another strategy"
   ],
   "id": "724ccd53be1e214f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def inf_density(classifier, X_pool):\n",
    "#     return information_density(X_pool, metric='euclidean')\n",
    "# \n",
    "# learner, metrics = train_active_learner(model_params=model_parameters, query_strat=inf_density,  epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)"
   ],
   "id": "c75a65b997a22566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "This is for using it with a Committee (for multiple classes) so it's not optimal for comparing the query strategies themselves maybe?"
   ],
   "id": "ea2db7db79104728"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "Acquisition Functions might not be usable due to the fact that they require a BayesianOptimizer, not an ActiveLearner"
   ],
   "id": "f2cbbd3fbf405c5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Acquisition Functions",
   "id": "f872cf1438ff1a5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Probability of improvement**: \n",
    "$$PI(x)=\\psi\\left(\\frac{\\mu(x) - f\\left(x^+\\right) - \\xi}{\\sigma(x)}\\right)$$\n",
    "where $\\mu(x)$ and $\\sigma(x)$ are mean and variance of regressor at $x$, $f$ is the model to be optimized with estimated maximum at $x^+$. $\\xi$ is a parameter controlling the degree of exploration and $\\psi(x)$ denotes cumulative distribution function of a standard Gaussian Distribution\n",
    "\n",
    "[Example from the ModAL Docs](https://modal-python.readthedocs.io/en/latest/_images/bo-PI.png)\n"
   ],
   "id": "ef26dec36c690061"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# learner, metrics = train_active_learner(model_params=model_parameters, query_strat=max_PI, epochs=epochs, random_seed=RANDOM_SEED, pool_idx=pool_idx, X_initial=X_initial, y_initial=y_initial)",
   "id": "b77668772d478e5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**[Expected Improvement (from the ModAL Docs)](https://modal-python.readthedocs.io/en/latest/content/query_strategies/Acquisition-functions.html#expected-improvement)**: \n",
    "$$\n",
    "EI(x) = \n",
    "\\left( \\mu(x) - f(x^+) - \\xi \\right) \\cdot \\psi \\left( \\frac{\\mu(x) - f(x^+) - \\xi}{\\sigma(x)} \\right)\n",
    "+ \\sigma(x) \\phi \\left( \\frac{\\mu(x) - f(x^+) - \\xi}{\\sigma(x)} \\right),\n",
    "$$\n",
    "\n",
    "where $\\mu(x)$ and $\\sigma(x)$ are the mean and variance of the regressor at $x$, $f$ is the function to be optimized with estimated maximum at $x$, $\\xi$ is a parameter controlling the degree of exploration, and $\\psi(z), \\phi(z)$ denote the cumulative distribution function and density function of a standard Gaussian distribution."
   ],
   "id": "571c93bd34cf6a53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Upper Confidence Bound**:\n",
    "$$ UCB(x) = \\mu(x) + \\beta \\sigma(x)$$\n",
    "where $\\mu(x)$ and $\\sigma(x)$ are mean and variance of the regressor and $\\beta$ is a parameter controlling the degree of exploration\n",
    "\n",
    "[Example from the ModAL Docs](https://modal-python.readthedocs.io/en/latest/_images/bo-UCB.png)"
   ],
   "id": "e8ebdeda7bf2d25e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
